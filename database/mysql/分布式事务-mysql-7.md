## 分布式事务-mysql-7

**分库分表中间件相当于把数据库解决不了的问题推到业务侧，让业务参与解决或者妥协**。随着技术发展，分布式数据库把上面的问题尽量的在数据系统内部解决掉，给客户的接口非常简单，统一的endpoint，标准的数据库协议，完整的sql支持能力，等等，但内部一样有各种数据分区逻辑。分布式数据库从广义上来说，就是实现数据库语义的分布式架构下的系统，像云上各种OLTP和OLAP产品，应该都可以称之为分布式数据库。分布式数据库中最重要的就是数据怎么摆放，数据在多个机器上平均分摊持有一份数据做sharding，还是多个节点相互复制一份数据做主备，还是利用底层共享存储共享一份完整数据集，衍生出不一样的系统架构和能力。



## 分布式数据库查询数据：疑惑

查找某条数据时，就使用日志中记录的分布式ID即可了，因为这个ID是唯一的。

但是，我要使用其他字段来查询怎么办呢？？？

用路由策略算，如果路由策略调整了怎么办，比如，之前有4个库，库的编号是1-4，之前路由算的结果可能是3，但是后来扩充了4个库变成8个库，编号为1-8、那么路由算出来的结果可能就是7了。

假如，每个库中表的数量都是10，那么可以保证表的编号不变，难道每次查询要连接所有的库去查询固定的表？？？ 好像这样做也行，挺好的，额外开销就是多建立了几个session连接，但是避免了扩容节点的时候，数据库数据的迁移。其实就是表变了，不确定在哪个表上了，很可怕。

例如在4张表的情况下，id为7的记录，7%4=3，因此这条记录位于user3这张表上。但是现在分表的数量变为了8个，而 7%8=0，而user0这张表上根本就没有id=7的这条记录，因此如果不进行数据迁移的话，就会出现记录找不到的情况。 

这个迁移挺麻烦的吧，要先拿出路由字段，根据4和8 分表 hash，如果一致则不变，如果不一致要从A节点将数据迁到B上面。

10亿条数据则需要计算10亿次？？？--



## 分布式事务ID



`Leaf`是美团推出的一个分布式ID生成服务，名字取自德国哲学家、数学家莱布尼茨的一句话：“There are no two identical leaves in the world.”（“世界上没有两片相同的树叶”），取个名字都这么有寓意，美团程序员牛掰啊！

`Leaf`的优势：`高可靠`、`低延迟`、`全局唯一`等特点。

目前主流的分布式ID生成方式，大致都是基于`数据库号段模式`和`雪花算法（snowflake）`，而美团（Leaf）刚好同时兼具了这两种方式，可以根据不同业务场景灵活切换。

在分库分表的情况下，数据库的自增主键已经无法使用。所以要使用一个分布式的id生成器。分布式事务id生成器要满足以下条件：唯一、趋势递增(减少落库时的索引开销)、高性能、高可用。

目前主流的分布式id生成方案都有第三方组件依赖，如：

- 基于zk
- 基于mysql
- 基于缓存

在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。如在美团点评的金融、支付、餐饮、酒店、猫眼电影等产品的系统中，数据日渐增长，对数据分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求；特别一点的如订单、骑手、优惠券也都需要有唯一ID做标识。此时一个能够生成全局唯一ID的系统是非常必要的。概括下来，那业务系统对ID号的要求有哪些呢？

1. 全局唯一性：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。
2. 趋势递增：在MySQL InnoDB引擎中使用的是聚集索引，由于多数RDBMS使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能。
3. 单调递增：保证下一个ID一定大于上一个ID，例如事务版本号、IM增量消息、排序等特殊需求。
4. 信息安全：如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定URL即可；如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要ID无规则、不规则。



## 弱依赖关系设计

但`Leaf-snowflake`对Zookeeper是一种弱依赖关系，除了每次会去ZK拿数据以外，也会在本机文件系统上缓存一个`workerID`文件。一旦ZooKeeper出现问题，恰好机器出现故障需重启时，依然能够保证服务正常启动。

类似，Mihoyo Takumi and Zest

## 动态扩容问题: 不迁移数据？

脑残吧，这个四位数的扩容，数据不迁移就需要维持就的路由表数据不变、

难道是，这个一长串ID就是分布式ID而且是事务执行前就生成了，并且这个ID里包含了库表字段？

然后，查找某条数据时，就使用日志中记录的这个ID即可了，因为这个ID是唯一的。



路由数据记录肯定不会存的，这样就不是去中心化了，因为10亿条时间的化，路由策略表要保存每条数据肯定是不可能的。 那么，修改了4 bits，怎么实现不需要后端数据迁移呢》？？

动态扩容指的是增加分库分表的数量。例如原来的user表拆分到2个库的四张表上。现在我们希望将分库的数量变为4个，分表的数量变为8个。这种情况下—般要伴随着数据迁移。例如在4张表的情况下，id为7的记录，7%4=3，因此这条记录位于user3这张表上。但是现在分表的数量变为了8个，而 7%8=0，而user0这张表上根本就没有id=7的这条记录，因此如果不进行数据迁移的话，就会出现记录找不到的情况。

卧槽，迁移的是迁移表，不是库？？？

4位数据库扩展位，保证了TB bits 不变，大不了去每个库中的制定表上执行select。

解决：

4位数据库扩展位∶ 为了实现不迁移数据的情况下，实现动态扩容，其中2位表示DB，2位表示TB，最多可扩容到10000张表。假设每张表存储1000万数据，则总共可以支持存储1000亿条数据。 



首先明确一点，路由策略始终根据数据库最后四位，确定某一条记录要到哪个分库的哪个分表中。例如xxxx0001，意味着这条记录肯定是在00分库的01分表上。

 id的生成策略上

假设初始状态为两个分库db 00，db 01，每个分库里面有10张分表，tb 00~tb 09。此时，业务要保证生成id的时候，始终保证db的两位在`00~01`之间，tb的两位始终在`00~09`之间。路由策略根据这些id，可以找到正确的分库分表。 现在需要扩容到10个分库，每个分库有10个分表。那么DBA首先将新增的分库∶db_02~db_09创建好，每个分库里面再创建10个分表∶ tb_01~tb_09。业务同学在此基础上，将id生成策略改成∶ db的两位在00~09之间，tb的两位规则维持不变（只是分库数变了，每个分库的分表数没变）。而由于路由从策略是根据最后四位确定到哪个分库，哪个分表，当这些新的分库分表扩展位id出现时，自然可以插入到新的分库分表中。

> query时，怎么保证旧的数据还能路由到旧的表呢？
>
> 是不是还要做路由更新。不然比如，id=1，数据过来了，本来路由到`00`库了，现在db生成的两位数在`00~09`了，会不会将id=1的数据判断到`08`库

也就实现了动态扩容，而无需迁移数据。 当然，新的分库分表中，一开始是没有数据的，所以数据是不均匀的，可以调整id扩展位中db和tb生成某个值的概率，使得落到新的分库分表中的概率相对大一点点（不宜太大），等到数据均匀后，再重新调整成完全随机。  





## 动态扩容：迁移数据必然hash

使用一致性hash实现添加节点,并进行数据迁移.数据迁移完成之前保留原有的节点路由信息.每次添加节点重新计算key值所在hash,hash到新节点的key可以先复制一份到新的节点,并标记旧节点的key待删除.直到所有的key都计算好迁移完毕,切换新旧节点信息,删除掉所有旧节点多余的key. 节点内的数据定位的话,先根据一致性hash确定所在节点,然后再根据节点自己的查找实现去定位数据,比如b-tree或者b+tree实现的文件系统.

## 业务层面分布式事务

Service A在执行某个操作时，需要操作数据库，同时调用Service B和Service C，Service B底层操作的数据库是分库分表的，Service C也要操作数据库。 

这种场景下，保证事务的一致性就非常麻烦。一些常用的一致性算法如∶ paxos协议、raft协议也无法解决这个问题，因为这些协议都是**资源层面的一致性**。在微服务架构下，已经将事务的一致性上升到了业务的层面

为了解决在事务运行过程中大颗粒度资源锁定的问题，业界提出一种新的事务模型，它是基于**业务层面**的事务定义。锁粒度完全由业务自己控制。它本质是一种**补偿**的思路。它把事务运行过程分成 Try、Confirm / Cancel 两个阶段。在每个阶段的逻辑由**业务代码**控制。这样就事务的锁粒度可以完全自由控制。业务可以在牺牲隔离性的情况下，获取更高的性能。

TCC 分别为 Trying，Confirm，Cancel 三个单词缩写，使用 2PC 或 3PC 实现的分布式框架，业务应用层无需改动，接入较简单。

> 2PC 和 3PC 事务由Transaction Manager负责、

作者：肚子很大干货很多的大飞
链接：https://juejin.cn/post/6914894669126533133

## 共识问题和原子提交问题：看

人们既然经常将Paxos、2PC、3PC这些算法放在一起讨论，那么它们之间势必存在着某种相似性的。但这种相似性是怎么来的呢？我们仔细分析一下。Paxos，是解决共识问题的通用算法。它允许每个节点提出自己的提议(称为proposal），而Paxos算法能够不借助于任何中心化节点，保证各个节点之间对于提议最终达成一致。这里的proposal，是一个抽象的概念，它可以包含任何你想达成共识的数值。2PC和3PC，则是为了解决分布式事务提交问题的。

这样从表面看起来，Paxos和2PC、3PC，这两类算法似乎没有多少相似性。2PC和3PC是跟分布式事务强相关的，而Paxos跟分布式事务没有什么特别的关系。

事务本来和分布式没什么直接关系的，就算在一个单节点的数据库上，要实现出事务的ACID特性，也不是那么容易的。

ACID中的原子性，要求事务的执行要么全部成功，要么全部失败，而不允许出现“部分成功”的情况。在分布式事务中，这要求参与事务的所有节点，要么全部执行Commit操作，要么全部执行Abort操作。换句话说，参与事务的所有节点，需要在“执行Commit还是Abort”这一点上达成一致（其实就是共识）。

这个问题在学术界被称为**原子提交问题**（Atomic Commitment Problem）[2]，而能够解决原子提交问题的算法，则被称为**原子提交协议**（Atomic Commitment Protocal，简称***ACP\***）[3]。2PC和3PC，属于原子提交协议两种不同的具体实现。

分析到这里，我们似乎发现了**原子提交问题**与**共识问题**的关联性：

- 共识问题，解决的是如何在分布式系统中的多个节点之间就某个提议达成共识。
- 原子提交问题，解决的是参与分布式事务的所有节点在“执行Commit还是Abort”这一点上达成共识。
- 所以，原子提交问题是共识问题的一个特例。

从两类问题各自的应用场景来看，这个差异是合理的，也是容易理解的。以解决共识问题的Paxos协议为例，它只要求网络中的大部分节点达成共识就可以了，这样Paxos才能提供一定的容错性，只要网络中发生故障的节点不超过一半仍然能够正常工作（不会被阻塞）。然而，解决原子提交问题的2PC或3PC则不同。即使只有一个节点发生故障了，其它节点也不能擅自决策进行Commit操作。因为这样的话，这个事务就只是「部分地执行成功了」，违反了ACID原子性的要求。所以，原子提交协议必须保证在参与分布式事务的**所有节点**（包括故障的节点）上对于“执行Commit还是Abort”达成共识。

故障的节点可能什么都做不了，如何参与达成共识呢？这里的意思是，等故障节点恢复之后，它的决策（Commit或是Abort）必须与其它所有节点保持一致。那么，这是不是意味着，只要有节点发生故障，原子提交协议就一定会阻塞呢？这里有点让人奇怪，答案是「不一定」。根源就在于Abort和Commit并不是对等的决策。假设有一个节点宕机了，其它节点大可以选择Abort决策（注意不能选择Commit），从而让整个事务Abort掉（没有被阻塞住，等待宕机的节点恢复）。等宕机的那个节点恢复了，它会发现相应的事务已经执行Abort了，那么它也按照Abort处理就好了。在这个过程中，参与分布式事务的**所有节点**（包括宕机的这个节点）对于“执行Commit还是Abort”也是达成了共识的（这个共识是Abort）。正是这些细微却至关重要的细节，让2PC和3PC这种看似简单的协议实现起来没有那么容易。



### 总结

当我们描述共识问题的时候，我们说的是在**多个节点**之间达成共识；而当我们描述原子提交问题的时候，我们说的是在**所有节点**之间达成共识。这个细微的差别，让这两类问题，几乎变成了完全不同的问题。

原子提交问题被抽象成一个新的一致性问题，称为uniform consensus问题，它是与通常的共识问题（consensus problem）不同的问题，而且是更难的问题。uniform consensus，要求所有节点（包括故障节点）都要达成共识；而consensus问题只关注没有发生故障的节点达成共识。

- 共识问题（consensus problem），解决的是如何在分布式系统中的多个节点之间就某个提议达成共识。它只关注没有发生故障的节点达成共识就可以了。
- 在分布式事务中，ACID中的原子性，引出了原子提交问题，它解决的是参与分布式事务的所有节点在“执行Commit还是Abort”这一点上达成共识。原子提交问题属于uniform consensus问题，要求所有节点（包括故障节点）都要达成共识，是比consensus问题更难的一类问题。
- Paxos和解决拜占庭将军问题的算法，解决的是consensus问题；2PC/3PC，解决的是一个特定的uniform consensus问题。





## CAP



## BASE





## 分布式事务模型/XA事务/DTP

XA 规范 是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准。

> X/Open DTP : X/Open Distributed Transaction Processing Reference

**XA（eXtended Architecture）**是指由X/Open 组织提出的分布式交易处理的规范。**XA 是一个分布式事务协议**，由Tuxedo 提出，所以**分布式事务也称为XA 事务**。**XA 协议主要定义了事务管理器TM（Transaction Manager，协调者）和资源管理器RM（Resource Manager，参与者）之间的接口**。

根据 Open Group 关于分布式事务的处理规范，定义了三种组件，如下图：

![img](https:////upload-images.jianshu.io/upload_images/7425654-20758d1b9efc6ce8.png?imageMogr2/auto-orient/strip|imageView2/2/w/935/format/webp)

其中

**AP**是指应用程序。

**RM**是资源管理器，事务的参与者，通常是数据库，比如MySQL Server。一个分布式事务通常涉及多个资源管理器。

**TM**是事务管理器，创建分布式事务并协调分布式事务中的各个子事务的执行和状态。子事务是指分布式事务中在RM上执行的具体操作。



**XA 事务允许不同数据库的分布式事务，只要参与在全局事务中的每个结点都支持XA 事务。Oracle、MySQL 和SQL Server 都支持XA 事务。**

**XA 事务由一个或多个资源管理器（RM）、一个事务管理器（TM）和一个应用程序（ApplicationProgram）组成。**

一般情况下，**某一数据库无法知道其它数据库在做什么，因此，在一个DTP环境中，交易中间件是必需的，由它通知和协调相关数据库的提交或回滚。而一个数据库只将其自己所做的操作（可恢复）影射到全局事务中。**

## 基于XA协议

XA 协议是由 X/Open 组织提出的分布式事务处理规范，主要定义了事务管理器 TM 和局部资源管理器 RM 之间的接口。

基于 XA 协议的二阶段提交协议方法和三阶段提交协议方法，采用了强一致性，遵从 ACID。

###2PC

看这个：https://matt33.com/2018/07/08/distribute-system-consistency-protocol/

只要第一阶段确定了的，第二阶段一定要按照第一阶段的确定结果执行提交或者回滚，也就是说，第一阶段，如果参与者都回复YES，那么第二阶段，所有参与者必须全部提交成功，并且只能成功不能失败，要保证只能成功，就必须要有重试机制，一次不行，俩次，这个重试时间越长就会导致资源占用越久，也就说**同步阻塞问题。**
2PC就是保证一致性的事物协议，如果第二阶段，发送断网，或者节点故障，那么在网络恢复后，或者节点恢复后，可以根据持久化的日志，继续执行第二阶段的提交，直到成功。典型的例子：mysql日志提交就是二阶段提交，binlog和redo log，就是靠2pc解决一致性的。

**(**Two-Phase Commit, 简称2PC**) ,**是为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法。分布式事务通常采用2PC，二阶段提交的算法思路可以概括为： 参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作，这里的参与者可以理解为RM，协调者可以理解为TM。如下图所示：

![img](https:////upload-images.jianshu.io/upload_images/7425654-b862882e7b10f282.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

在第一阶段，TM会发送 Prepare 到所有参与分布式事务的RM询问是否可以提交操作，参与分布式事务的所有RM接收到请求，实现自身事务提交前的准备工作并返回结果。

在第二阶段，根据RM返回的结果，如果涉及分布式事务的所有RM都返回可以提交，则TM给RM发送commit的命令，每个RM实现自己的提交，同时释放锁和资源，然后RM反馈提交成功，TM完成整个分布式事务；如果任何一个RM返回不能提交，则涉及分布式事务的所有RM都被告知需要回滚。MySQL XA 也是基于这个规范实现的。

二阶段提交协议存在的最明显也是最大的一个问题就是同步阻塞，这会极大地限制分布式系统的性能

协调者单点问题，协调者的角色在整个二阶段的提交协议中起到了非常重要的作用。

二阶段提交协议没有设计较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败。

#### 2PC数据不一致情况

Coordinator 和参与者在第二阶段挂了，挂的这个参与者在挂之前已经执行了操作。但是由于他挂了，没有人知道他执行了什么操作。

这种情况下，新的 Coordinator 被选出来之后，如果他想负起 Coordinator 的责任的话他就只能按照之前那种情况来执行 commit 或者 roolback 操作。这样新的 Coordinator 和所有没挂掉的参与者就保持了数据的一致性，我们假定他们执行了 commit。但是，这个时候，那个挂掉的参与者恢复了怎么办，因为他已经执行完了之前的事务，如果他执行的是 commit 那还好，和其他的机器保持一致了，万一他执行的是 roolback 操作呢？这不就导致数据的不一致性了么？虽然这个时候可以再通过手段让他和 Coordinator 通信，再想办法把数据搞成一致的，但是，这段时间内他的数据状态已经是不一致的了！

emm，挺极端的

简单说就是，数据不一致：这个在前面已经讲述过了，如果在第二阶段，Coordinator 和参与者都出现挂掉的情况下，是有可能导致数据不一致的。

### 3PC

三阶段提交就有：CanCommit、PreCommit 和 DoCommit 三个阶段。

> PreCommit  阶段，各个节点数据已经保持一致了。
>
> 参与者接收到 preCommit 请求后，会执行事务操作，并将 Undo 和 Redo 信息记录到事务日记中；
>
> 参与者收到 doCommit 请求后，正式提交事务，并在完成事务提交后释放占用的资源；

为了解决两阶段提交在协议的一些问题，三阶段提交引入了超时机制和准备阶段，如果协调者或者参与者在规定的之间内没有接受到来自其他节点的响应，就会根据当前的状态选择提交或者终止整个事务，准备阶段的引入其实让事务的参与者有了除回滚之外的其他选择。

与两阶段提交不同的是，三阶段提交有两个改动点：

1）引入超时机制。同时在协调者和参与者中都引入超时机制。

2）在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。

为了解决两阶段提交的问题，三阶段提交做了改进：

在协调节点和事务参与者都引入了超时机制。
第一阶段的 prepare 阶段分成了两步，canCommi 和 preCommit。
如下图：

![image.png](https://segmentfault.com/img/remote/1460000039910625)



引入 preCommit 阶段后，协调节点会在 commit 之前再次检查各个事务参与者的状态，保证它们的状态是一致的。但是也存在问题，那就是如果第三阶段发出 rollback 请求，有的节点没有收到，那没有收到的节点会在超时之后进行提交，造成数据不一致。

#### 3PC数据不一致情况

3PC 虽然解决了 Coordinator 与参与者都异常情况下导致数据不一致的问题，3PC 依然带来其他问题：比如，网络分区问题，在 preCommit 消息发送后突然两个机房断开，这时候 Coordinator 所在机房会 abort, 另外剩余参与者的机房则会 commit。

而且由于3PC 的设计过于复杂，在解决2PC 问题的同时也引入了新的问题，所以在实际上应用不是很广泛。







## TCC： 需要大量应用代码改造

https://juejin.cn/post/6844903647197806605

看：https://www.cnblogs.com/jajian/p/10014145.html

业务场景有了，现在我们要更进一步，实现一个 TCC 分布式事务的效果。

什么意思呢？也就是说，[1] 订单服务-修改订单状态，[2] 库存服务-扣减库存，[3] 积分服务-增加积分，[4] 仓储服务-创建销售出库单。

上述这几个步骤，要么一起成功，要么一起失败，必须是一个整体性的事务。

举个例子，现在订单的状态都修改为“已支付”了，结果库存服务扣减库存失败。那个商品的库存原来是 100 件，现在卖掉了 2 件，本来应该是 98 件了。

结果呢？由于库存服务操作数据库异常，导致库存数量还是 100。这不是在坑人么，当然不能允许这种情况发生了！

但是如果你不用 TCC 分布式事务方案的话，就用个 Spring Cloud 开发这么一个微服务系统，很有可能会干出这种事儿来。

TCC 分布式事务，必须引入一款 TCC 分布式事务框架，比如国内开源的 ByteTCC、Himly、TCC-transaction。

否则的话，感知各个阶段的执行情况以及推进执行下一个阶段的这些事情，不太可能自己手写实现，太复杂了。

TCC：

* Try

  这个操作，一般都是锁定某个资源，设置一个预备类的状态，冻结部分数据，等等，大概都是这类操作。

* Confirm

  依次调用各个服务的 Confirm 逻辑。然后，正式完成各个服务的所有业务逻辑的执行。

* Cancel

   Try 阶段，比如积分服务吧，它执行出错了，此时会怎么样？

  那订单服务内的 TCC 事务框架是可以感知到的，然后它会决定对整个 TCC 分布式事务进行回滚。

  也就是说，会执行各个服务的第二个 C 阶段，Cancel 阶段。同样，为了实现这个 Cancel 阶段，各个服务还得加一些代码

这个应用层面支持有好多--

要玩儿 TCC 分布式事务的话：首先需要选择某种 TCC 分布式事务框架，各个服务里就会有这个 TCC 分布式事务框架在运行。

然后你原本的一个接口，要改造为 3 个逻辑，Try-Confirm-Cancel：

- 先是服务调用链路依次执行 Try 逻辑。
- 如果都正常的话，TCC 分布式事务框架推进执行 Confirm 逻辑，完成整个事务。
- 如果某个服务的 Try 逻辑有问题，TCC 分布式事务框架感知到之后就会推进执行各个服务的 Cancel 逻辑，撤销之前执行的各种操作。



## Seata(厉害)



http://seata.io/zh-cn/blog/seata-xa-introduce.html

流弊，https://www.cnblogs.com/chengxy-nds/p/14046856.html



`Seata` 是两段提交，Seata 是一个需独立部署的中间件，所以先搭 Seata Server，这里以最新的 `seata-server-1.4.0` 版本为例，下载地址：`https://seata.io/en-us/blog/download.html`

https://www.cnblogs.com/chengxy-nds/p/14046856.html

 `Seata` 分布式事务的几种角色：

- `Transaction Coordinator(TC)`: 全局事务协调者，用来协调全局事务和各个分支事务（不同服务）的状态， 驱动全局事务和各个分支事务的回滚或提交。
- `Transaction Manager™`: 事务管理者，业务层中用来开启/提交/回滚一个整体事务（在调用服务的方法中用注解开启事务）。
- `Resource Manager(RM)`: 资源管理者，一般指业务数据库代表了一个分支事务（`Branch Transaction`），管理分支事务与 `TC` 进行协调注册分支事务并且汇报分支事务的状态，驱动分支事务的提交或回滚。



### ＸＡ事务模式

* TM 开启全局事务
* RM 向 TC 注册分支事务
* RM 向 TC 报告分支事务状态
* TC 向 RM 发送 commit/rollback 请求
* TM 结束全局事务

### AT事务模式

AT 模式是一种无侵入的分布式事务解决方案。在 AT 模式下，用户只需关注自己的“业务 SQL”，用户的 “业务 SQL” 作为一阶段，Seata 框架会自动生成事务的二阶段提交和回滚操作。

AT模式实质是两阶段提交协议的演变，具体如下：

- 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源
- 二阶段：
  提交异步化，非常快速地完成。

回滚通过一阶段的回滚日志进行反向补偿。

https://www.cnblogs.com/chengxy-nds/p/14046856.html

https://developer.aliyun.com/article/768398

### TCC事务模式

TCC 模式需要用户根据自己的业务场景实现 Try、Confirm 和 Cancel 三个操作；事务发起方在一阶段执行 Try 方式，在二阶段提交执行 Confirm 方法，二阶段回滚执行 Cancel 方法。

### SAGA事务模式

https://www.sofastack.tech/blog/sofa-meetup-3-seata-retrospect/

Saga 是一种补偿协议，在 Saga 模式下，分布式事务内有多个参与者，每一个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向操作和逆向回滚操

Saga 模式适用于业务流程长且需要保证事务最终一致性的业务系统，Saga 模式一阶段就会提交本地事务，无锁、长流程情况下可以保证性能。

事务参与者可能是其它公司的服务或者是遗留系统的服务，无法进行改造和提供 TCC 要求的接口，可以使用 Saga 模式。

Saga模式的优势是：

- 一阶段提交本地数据库事务，无锁，高性能；
- 参与者可以采用事务驱动异步执行，高吞吐；
- 补偿服务即正向服务的“反向”，易于理解，易于实现；

缺点：Saga 模式由于一阶段已经提交本地数据库事务，且没有进行“预留”动作，所以不能保证隔离性。后续会讲到对于缺乏隔离性的应对措施。



## Mysql XA事务

https://www.jianshu.com/p/7003d58ea182

Mysql 的XA事务分为内部XA和外部XA。 外部XA可以参与到外部的分布式事务中，需要应用层介入作为协调者；内部XA事务用于同一实例下跨多引擎事务，由Binlog作为协调者，比如在一个存储引擎提交时，需要将提交信息写入二进制日志，这就是一个分布式内部XA事务，只不过二进制日志的参与者是MySQL本身。

来源：https://twgreatdaily.com/zh-cn/9l5Ns20BMH2_cNUgyNDR.html

###基本用法

```sql
#  开启一个XA事务
XA {START|BEGIN} xid [JOIN|RESUME]

# 结束一个XA事务操作
XA END xid [SUSPEND [FOR MIGRATE]]

# 准备提交一个XA事务
XA PREPARE xid

# 正式提交一个XA事务
XA COMMIT xid [ONE PHASE]

# 回滚一个XA事务
XA ROLLBACK xid

# 列出所有处于prepared状态的XA事务
XA RECOVER [CONVERT XID]
```

任何`XA`语句都以`XA`关键字开头，大多`XA`语句都需要`xid`值。`xid` 是 **XA事务的标识符** ，它确定语句应用到哪个XA事务上。

`xid`值可以由客户端指定或 MySQL 服务器生成。

一个`xid`值有一到三个部分：

```
xid: gtrid [, bqual [, formatID ]]
```

`gtrid` 是**全局事务标识符** ，`bqual` 是**分支修饰符**，`formatID`是一个标记 `gtrid` 和 `bqual` 格式的数字。

`XA RECOVER` 语句返回 MySQL 服务器中处于 `PREPARED` 状态的 XA 事务信息。输出中每一行都是一个服务器上的 XA 事务，不论是哪个客户端启动的事务。

执行 `XA RECOVER` 需要 `XA_RECOVER_ADMIN` 特权。这个特权需求是为了防止用户发现其他不属于自己的事务`xid`，不影响XA事务的正常提交和回滚

### 事务状态

一个 XA 事务经历以下状态

1. 使用`XA START`启动的XA事务，进入`ACTIVE`状态。

2. 一个处于`ACTIVE`状态的XA事务，可以发出SQL语句填充事务，然后发出`XA END`语句。`XA END`语句令XA事务进入`IDLE`状态。

3. 一个处于

   `IDLE`状态的XA事务，可以发出`XA PREPARE`语句或`XA COMMIT ... ONE PHASE`语句。

   - `XA PREPARE` 语句令XA事务进入`PREPARED` 状态。`XA RECOVER` 语句此时可以发现并列出此事务的 XID。`XA RECOVER` 可以列出所有处于 `PREPARED` 状态的 XA 事务的 XID。
   - `XA COMMIT ... ONE PHASE` 准备并提交XA事务。`xid`不会列出在`XA RECOVER`中，因为XA事务实际在执行语句后就结束了。

4. 一个处于`PREPARED`状态的XA事务，可以发出`XA COMMIT`语句来提交并结束XA事务，或发出`XA ROLLBACK`来回滚并结束事务。

### XA事务实验

```sql
mysql> create database if not exists test;
ERROR 1290 (HY000): The MySQL server is running with the --super-read-only option                                                             so it cannot execute this statement
mysql> select @@read_only;
+-------------+
| @@read_only |
+-------------+
|           1 |
+-------------+
1 row in set (0.00 sec)

mysql>  set global read_only=0;
Query OK, 0 rows affected (0.00 sec)

mysql> select @@read_only;
+-------------+
| @@read_only |
+-------------+
|           0 |
+-------------+
1 row in set (0.00 sec)

mysql> create database if not exists test;
Query OK, 1 row affected (0.01 sec)


mysql> use test;
Database changed
mysql> create table if not exists test123 (
    ->   `id` bigint primary key auto_increment,
    ->   `name` varchar(64) not null
    -> );
Query OK, 0 rows affected (0.03 sec)

mysql> xa start 'this-is-gtrid','this-is-bqual';
Query OK, 0 rows affected (0.00 sec)

mysql> insert into test123(name) values('distributed transaction!');
Query OK, 1 row affected (0.01 sec)

mysql> xa end 'this-is-gtrid','this-is-bqual';
Query OK, 0 rows affected (0.00 sec)

mysql> xa prepare 'this-is-gtrid','this-is-bqual';
Query OK, 0 rows affected (0.01 sec)

mysql> xa recover;
+----------+--------------+--------------+----------------------------+
| formatID | gtrid_length | bqual_length | data                       |
+----------+--------------+--------------+----------------------------+
|        1 |           13 |           13 | this-is-gtridthis-is-bqual |
+----------+--------------+--------------+----------------------------+
1 row in set (0.00 sec)

mysql> xa commit 'this-is-gtrid','this-is-bqual';
Query OK, 0 rows affected (0.00 sec)

mysql> select * from test123;
+----+--------------------------+
| id | name                     |
+----+--------------------------+
|  1 | distributed transaction! |
+----+--------------------------+
1 row in set (0.00 sec)

```

end

## 分库分表

分库分表应该注意热点数据。

148632

## HotDB

不同业务分开到不同的DRDS集群

分集群原因：

计算节点受到sql的影响，应该把核心业务和边缘业务分开。

避免边缘业务解析复杂sql的时候，影响核心业务的使用。



注意点

* RAID 10 不好，磁盘相互同步，损坏率太高，且容量变小，使用RAID5就行了
* 暴力关机测试，一定要在机器装好后就测试。避免有硬盘等问题

