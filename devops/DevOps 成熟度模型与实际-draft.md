# DevOps 成熟度模型与实际

## 概述

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/devops整体架构.jpg)

广义的DevOps概念很广泛

运维侧来看狭义DevOps会聚焦于CICD流程。

## 敏捷开发

敏捷开发是体系和文化，

DevOps工具链和文化的结合比较明显，我主要负责DevOps工具链的实现。

几个部分：

代码质量，集成策略和流程，发布流程。

业务发言权还是研发发言权即谁决定发布窗口

### 标准化、自动化、流程化

docker, 标准化
各种工具，自动化 
DevOps， 流程化

DevOps，70%文化和改变，30%工具和实践。

### 传统开发：期望需求分析是不可变的

哈哈哈，果然，如果产品定位明确，需求分析稳定，根本不需要要敏捷。

要理解敏捷开发，最好的切入点是知道：什么不是敏捷开发？

早些年，我参与过一个不小的项目，客户是一著名电视台。系统是用来支持当时大火的选秀节目，具体模块有场外观众短信支持，候选人得票情况的可视化显示，现场嘉宾的打分等子模块。

总之，是一个功能蛮多，交互蛮复杂的系统。

因为在当年，选秀是一个新形态，所以作为开发人员，当然也不知道客户（电视台）想要的是什么东西。但最糟糕的是，连电视台自己也不知道想要的东西具体长啥样？而且因为是搞艺术的，想象力自然丰富，因此提的需求大概就是：

“要酷，要震撼，要能调动观众的积极性...”等等。总之就是需求很模糊，说不清楚。

但当时还不流行所谓的“敏捷开发”，常规的开发流程如下：

1. 需求分析
2. 设计
3. 编码
4. 测试
5. 交付

这也就是我们常说的“**瀑布模型**”。

可以现象，在该流程下，居于核心的就是需求分析。一旦需求分析出现大的偏差，之后的设计、编码、测试，即使做的再好，也是徒劳。因为你最后交付的根本不是用户想要的。

但是软件是这种东西，因为看不见摸不着。因而在具体的东西出来之前，大概率，用户根本就不知道自己想要的是什么。就好像电视台的工作人员，只知道 “酷、炸、炫”是他的需求，但如果再往具体处问，就不知道该说啥了。

但没办法，公司要挣钱，项目就一定要推下去，因此需求调研人员只能硬着头皮上。

虽然和电视台人员没少聊，但总体来说，也是言辞不详。最后的结果是，参考电视台的说辞，加上自己的猜测，再参考国外同类节目，晕晕沉沉弄了一个《需求分析》出来。

然后把《需求分析》传给了电视台，也不知道对方到底认真看了没有，反正最后的回复是：“没错，就是它，尽快出东西！”。

于是接下来，大家加班加点2个月，第一版终于出来了。欢天喜地给电视台看，本来大家的期望是掌声和赞美。但没想到，对方看了之后非常失望，说根本就不是他想要的东西。我们和对方争辩说，《需求分析》你也认可了，怎么可能不是你想要的东西呢？

结果对方也很委屈，答道：“《需求分析》我是看了，但这肯定不是想要的东西，你看这点..，这点...。”

原来，就这个《需求分析》来说，没想到同样的文字，不同人真会有不同的理解，甚至是完全相反的理解。

但这就是软件开发，因为看不见摸不着，高度定制，没有工业化标准。因此对用户来说，直到第一次看到实物（可运行的软件），才会逐渐在脑子中清晰他想要的是什么的东西。这就是软件开发的本质所在，没有任何人真的有错。

但当时并不明白这个道理。

虽然很生气，奈何项目不小，对方又居于绝对强势。不爽归不爽，但回过头来，只有继续加班加点，吭哧吭哧的重新设计，重新编码。

因此，一个最初预期30个人月的项目，最终竟投入了100个人月还不止。

好在无论如何，最后，客户还是认可了我们的产品。但其中付出的辛苦，只有在身在其中，才会有真正明白。

当然，人吃一个亏，总归会有些进步的。

因此，为避免再出现《需求分析》争议的情况，在之后项目中，在需求分析阶段，如果有可能，我们就会尽量的在《需求分析》中增加图示。如果项目不大，甚至会先做出一个原型出来。

当然，这些做法效果肯定是有的。但也并没有从根本上解决问题。之后的项目还是会有返工的情况出现，区别只是严重程度的大小而已。

而我后来明白，这种情况（返工），只要是在“瀑布模型”下，就无法根本避免，因为来自于以下事实：

1. 只要有沟通，就会有信息的变形，因此《需求分析》是不可靠的。
2. 即使《需求分析》是可靠的，随着项目的进展，竞争对手的出现，需求也可能过时。
3. 瀑布模型，这种自上而下的开发方式，无法响应这个快速变化的时代。

而要解决这个问题，就必须引入更轻便的开发方式，这就是“敏捷开发”。

但是普通人可能没想到，敏捷开发并没有定义具体的开发过程，而是起源于一个简单的理念（确实够敏捷的），这就是《敏捷宣言》：

> 我们一直在实践中探寻更好的软件开发方法，身体力行的同时也帮助他人。由此我们建立了如下价值观：
>
> 个体和互动 高于 流程和工具；
> 工作的软件 高于 详尽的文档；
> 客户合作 高于 合同谈判；
> 响应变化 高于 遵循计划；
>
> 也就是说，尽管右项有价值，我们更重视左项的价值。

而之后的各种方法论，其实都是为了践行这个原则。

> 敏捷冲刺、每日站会等 这些都是敏捷方法论

但总的来说，软件首先是“软”的，要解决根本问题，还是要靠其中的人（产品、测试、开发）。因为软件是“软”的，则务必要求软件的从业人员也是“软的”，从这个角度出发，软实力是每个软件从业人员走向优秀的必备之道。



作者：沈世钧
链接：https://www.zhihu.com/question/19645396/answer/608940699
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

### gitlab-ce and gitlab-ee

#### merge request

merge时，在Describe栏中填写` +r @jiangshengyang`.

#### 功能对比

官方给的例子
https://gitlab.com/gitlab-org/gitlab/-/merge_requests



对比：
https://about.gitlab.com/install/ce-or-ee/
features：https://about.gitlab.com/pricing/self-managed/feature-comparison/
ce：只能Assignee一个人



### 敏捷：需求分析随时变

从本质上讲，敏捷（Agile）并不是开发方法，而是一种理念。对于项目管理而言，敏捷是一个全新的术语，敏捷强调在软件研发过程中持续性的根据用户反馈和需求优先级来发布新版本，不断进行迭代，让产品逐渐完善。

总结为：

1. *以人为本：重视个体间的合作互动*
2. *目标导向：我们最终交付的是“可使用的软件”，而不是一堆繁重的文档*
3. *客户为先：理解客户需求，与客户合作*
4. *拥抱改变：客户会在不断变化需求的过程中明晰真正需要的，因此敏捷需要拥抱变化*



#### 敏捷开发的应用场景：不明确

即所有人没有明确的产品定位，不断的随着市场或客户进行调整，在不断的反馈改变中，快速高效的调整开发方向，从而形成最终市场或客户接纳的产品。

敏捷项目管理中有句话叫做：快速失败。

比如我们接手了一个连最终产出都不明确的项目，首先我们会先交付最小模型产品，这时我们得做好被质疑的准备。毕竟没人知道要做出怎么样的产品，所以我们的最小模型的产品很可能是个怪胎。在与客户反复测试后，我们会才会逐步了解他们的真实需求，这时候我们离成功又近了一步。

#### 迭代管理

这个很重要：看板、燃尽图等



#### 敏捷开发实践

腾讯/mihoyo，每天早上的10点的站会... 恶心但是有用？？？

每日站会--：

因为敏捷开发强调沟通，因此团队就必须提供时间和场景供大家沟通，每日站会因此而生。

在站会上，任何人（产品经理，测试、工程师）都可以分享昨天他干了什么？有什么困难？有什么需要帮助的？以及...

总之，就是通过及时的交流，把任何问题解决于无形之中。



**首先，敏捷开发是一种过程控制论，通俗的说，就是****一种做事情的方法。**

\1. 它适用于软件，因为软件是软的，可以改。要是硬件，改起来就没那么方便了
\2. 它适用于客户不知道自己要啥的情况，其实，这样的客户占绝大多数。因为客户不知道要啥，所以你需要不断帮客户弄明白他到底想要啥。。。换句话说，你需要和客户沟通，合作，倾听反馈，持续改进。。。
\3. 它适用于竞争激烈的市场，这样的情况下，赶在竞争对手前交付一个不完美但至少能用的产品非常重要。
\4. 它适用于快速变化的市场，你在埋头造一辆汽车的时候，客户已经想开飞机满天飞了，这就需要你能一步步的把汽车改成飞机，还能按时交付。
\5. 它适用于在一个地方办公的小团队，一般10个人以内。这样能使敏捷中主要的沟通方式“Face to Face” 是可行的。

**其次，敏捷开发是一套工具集，里面有形形色色的工具，你可以不搞敏捷，但可以用那么一两个来提高工作效率。**

比如：
\1. 站会：三个问题，简洁有效的小团队沟通方式
\2. 看板：直观反映工作进度，反映流程遵守情况，反映流程缺陷
\3. 演示，计划，反思会：适合于小团队的协作和优化反馈方式
\4. 用户故事：站在用户的角度讲需求
\5. 持续集成：随时高质量交付的基础，有利于应对变化剧烈的市场

**再其次，敏捷开发是一种企业管理方式**

比如：
\1. 一线员工可以同时是架构师，Scrum Master，开发工程师，测试工程师，发挥了他的主观能动性，有利于创新和效率
\2. 敏捷不专注于敏捷团队中个人的绩效考核，而更多的侧重于整个团队的绩效，更好的避免了KPI驱动模式。
\3. 把大项目拆分成小项目去做（每个Sprint都是一个迭代，需要输出一个高质量的版本，相当于完成一个小项目），把bug的生存期控制在一个迭代以内，降低了风险，也减少了后期改bug的工作量。
\4. 把数十人的大team 分成几个敏捷团队，这几个敏捷团队的Scrum Master/PO再组成一个更高一级的敏捷团队，利用站会，反思，看板等等敏捷元素，可以避免数十份邮件也不能解决一个小问题，大家互相踢皮球，沟通不畅的大企业病。
\5. 老板可以是最大的PO，他给下面的高管讲idea(User Story)，定期检查Demo，把控产品用户体验，负责和外界的沟通合作-----比如乔布斯，360的周鸿祎等

作者：付聪
链接：https://www.zhihu.com/question/19645396/answer/16635773
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

#### 故事点Story Point：先确定工作总量在求工作时间

https://segmentfault.com/a/1190000039854092

《人月神化》有类似的这么一个比喻：一个项目估算出需要12人月，如果有12个人，换算过来，一个月就能完成。现在人们常用的一个比喻是，9个孕妇不可能一个月生出一个孩子！这都是对估算和计划的调侃。



**「故事点」是敏捷开发中一种有效的度量单位，它以数字的形式呈现，表示完成某个用户故事开发所需要的工作量**。

与「工时」不同，「故事点」是一个抽象的、相对的值，**它包含了对开发任务量、复杂度、风险和不确定性的整体预估**。然而，由于每个人/团队的技术水平存在差异，对同一任务的复杂性和风险程度的判断也是不同的，因此每个团队对「故事点」都有自己的标准。**一旦团队对「故事点」达成了共识，它能够帮助各个成员在估算工作量时快速达成一致，并有效地衡量团队产能**。

**为了确定故事点的标准，团队需要先找到一个基准故事**，该基准故事需包含解决具体用户故事所要完成的标志性任务，例如选择一个包含前端和后端任务，后端有数据信息交互的用户故事作为基准故事，其工作量设为1个故事点，那么其他用户故事则可以基于这一基准故事进行故事点的预估。比如某团队设置基准故事 A 为1个故事点，用户故事 B 的开发任务量、复杂度、风险和不确定性综合预估是基准故事 A 的3倍，那么用户故事 B 的故事点就应该设立为3。

故事点的取值需遵循斐波那契数列数列（1、2、3、5、8、13、21、34...), 为了避免繁琐，更好的体现故事点的差异性和准确性，团队可沿用修正版的斐波那契数列（1、2、3、5、8、13、20、40...）。





故事点，以需求推导项目的deadline，而不是以项目deadline去压缩需求，做到真正的需求评审，在迭代时间点内根据需求紧急程度来安排具体来做哪个需求。

两个程序员也许同意一个用户故事是5个故事点。高级开发人员可能觉得这很容易，**1天**（时间，或者精确到8小时-）就能完成。初级开发人员可能觉得需要**2天**才能搞定。但是他们能在5个故事点上快速达成一致，因为把第一个用户故事定成5个故事点是不需要什么依据的（就像不同国家或地区对长度单位的定义可能不同，比如米、寸、英寸等，每个团队对故事点的大小也都可以有自己的标准，只要团队成员都认可即可）。

不过，最重要的是，一旦他们同意第一个故事的工作量是5个点，这两个开发人员就可以对后续估算达成共识。

如果高级开发人员认为一个新的用户故事将需要两天（两倍于他估计为5个点的故事），他将评估新的故事为10个点。而初级开发人员也会这样做，当他估算需要4个工作日时（两倍于他估计为5个点的故事），他将评估新的故事为10个点。

所以，使用故事点而不使用时间的原因是，故事点可以使能力不同的开发人员在估算同一任务时快速达成一致。

先基于第一个用户故事，团队达成一个故事点的认知即一个需求需要多少个故事点来完成，这样后续的需求都可以根据第一个故事点来推断其他故事点数。这样可以保证以一个项目和一个大需求，可以被拆分成团队认知一直的故事点数，这样可以量化，也可以屏蔽掉个人能力差异性导致的整体项目时间不好把控。

它能使不同技术水平和工作速率的人在工作量的估算上快速达成一致。

核心：项目经理根据故事点，将一个项目或者一个版本的大需求，拆分成S个故事点。让询问每个开发人员完成一个故事点的时间m1...mN，然后(S/m1)+...+(S/mN)=N。即可得到整个项目或版本的时间节点。

或者，可以直接计算每个研发人员每月完成了多少故事点来考核产出--

#### 缺陷

- 软件缺陷，通常又被叫做Bug或者defect,即为软件或程序中存在的某种破坏正常运行能力的问题、错误、其存在会导致软件产品在某种程度上不能满足用户的需求。
- 软件缺陷是指存在于软件（程序、数据、文档）中的那些不符合用户需求的问题。

#### 非功能需求

##### 性能需求

响应时间、吞吐量和资料利用率

##### 安全性

保密性、防泄漏、权限控制和防攻击

##### 可维护性与可扩展性

模块性、可复用性和易分析性

##### 可靠性

易恢复性、容错性和成熟型

##### 易用性





### TAPD

TAPD使用经验

#### 项目

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/项目-tapd.jpg)

#### 迭代/Sprint

迭代（Sprints），我将其理解为通过短期研发完成具体任务来达到目标的一个过程，也是帮助产品经理和研发团队逐渐切入项目细节的方法。

通常情况下，每次迭代大约要花费1-4周。具体的时长我们需要根据团队过往的表现情况来制定，同时尽量保持每次迭代的时长相同。

迭代是整个团队的活，因此，产品经理、项目经理以及其他所有成员都该积极参与其中，表达自己的声音和想法。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/迭代-1-tapd.jpg)

点击迭代 --> 详情，可以看到总执行度。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/迭代-2-tapd.jpg)

#### 需求/用户故事

企业版还有task

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/需求-tapd.jpg)

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/user-story-用户故事-devops.jpg)

#### 测试用例

每个版本添加N个测试用例，发版之前执行对应测试用例，验证版本功能。

#### CICD and Jira/禅道/Tapd

jira、禅道 与 devops 对接 是个大头、。
提交必须与 task或者用户故事对接管理。
根据需求开发 -- 不能随意提交，

米哈游，哲哥后来大块的精力也是在做这块，将每次gitlab commit和Tapd的task或者需求（user story）

## CICD：减少人为工作而不是减少流程

业务、架构、技术，人、流程、工具，原则、方法、实践，这九大元素不能孤立的来看，原本就是相辅相成，密切相关的。

Principle背后，其实是人的Mindset思维模式，而一堆人遵循同样的Principle，就演化成了文化Culture。方法Method也好，流程Process也好，最终都由实践Practice通过工具Tool落地。

DevOps、微服务和容器的三剑客，也是方法、架构与技术与工具的极佳结合。

技术、工具、实践，都是服务于方法和流程的，需要遵循核心的原则，最终的目的是为了商业的诉求，为了快速的价值交付。

敏捷是为了解决业务与开发之间的鸿沟。通过敏捷宣言中强调的个体和互动、可工作的软件、客户合作、响应变化，以及12条原则中的尽早的以及连续的高价值交付、自组织团队、小批量交付、团队节奏、可改善可持续的流程、保持沟通等，以及包括Scrum、Kanban、XP、TAPD在内的众多管理和工程实践，来实现开发与业务之间的频繁沟通，快速响应变化。

DevOps的出现，是为了开发与运维之间的鸿沟。业务代码的敏捷的确是快了，却发现因为Dev与Ops之间的隔阂，无法真正的将价值持续的交付给客户。开发（尤其是“敏捷”后），求的是快速响应变化；运维，求的是稳定、安全和可靠的服务。更重要的，两者的KPI度量指标，绩效考核激励机制不同，决定了如果为达成各自的局部目标，势必存在无法调和的根因冲突。

狭义的DevOps概念，解决的是Dev到Ops的鸿沟。E2E，End to End，即端到端、广义的DevOps，是以精益和敏捷为核心的，解决从业务到开发到运维，进而到客户的完整闭环

> DevOps要实现：自动化、标准化、配置化。

DevOps，即**Development and Operations**，是一组过程、方法与系统的统称，用于促进软件开发、运维和质量保障部门之间的沟通、协作与整合。DevOps可看作开发、运维和质量保障（QA）三者的交集。



持续交付是狭义DevOps的核心理念，横跨了架构、开发、测试、运维等角色。持续交付的核心开发实践，也涵盖了架构管理、版本管理、分支策略、测试自动化、部署发布、运维监控、信息安全、团队授权、数据库管理等多个维度。

DevOps工具链：人工的动作基本上都消除后，接下来就是去提高工具流里面各个环节的执行效率和环节之间的衔接效率了。

#### 持续集成

gitlab的群组和子群组功能，挺好用的。

持续集成的工作原理是将小的代码块推送到Git仓库中托管的应用程序代码库中，并且每次推送时，都要运行一系列脚本来构建、测试和验证代码更改，然后再将其合并到主分支中。

随着敏捷开发的发展，持续集成在软件项目活动中也日益成为主流。顾名思义，持续集成是指每日频繁地（比如一天多次）将代码集成到主干分支中。强调通过集成和测试的速度，快速给出一个集成的结果（是失败还是成功），在代码集成之前，必须先通过自动化测试验证，只要有一个测试用例失败，就不能集成。

持续集成是和单元测试结合在一起的，也就意味着，持续集成和单元测试需要并行工作。持续集成一般由代码每次 git push/review 触发。

持续集成并不能消除Bug，而是让它们非常容易被发现和改正。

集成与持续，分别从空间和时间两个维度定义了CI的两个核心实践：

主干开发与小批量高频提交。 良好的持续集成 =主干开发与小批量高频提交（人的因素） + 一定程度的自动化（工具的因素）。

人是践行CI实践的核心一环，人才是核心因素，工具与自动化解决效率问题，以人的实践为前提并起重要作用。

持续集成的工作阶段比较明确，主要有三个大的阶段：持续集成准备阶段、持续集成使用阶段和持续集成测试阶段。

持续集成准备阶段的工作主要包括：

- 通过代码评审系统（比如 Gerrit），实现代码审查和集成反馈。
- 通过版本控制系统（比如 Git或 GitLab）建立源码仓库。
- 通过构建工具运行相关构建和测试（比如 Python 项目的 Tox 和 Pytest）。
- 通过 CI 系统（比如 Jenkins）建立 Job，将版本控制和构建工具整合，并设置构建触发条件。

持续集成使用阶段的工作主要包括：

- 开发人员向代码评审系统（比如 Gerrit）提交代码。
- 通过 CI 系统监听代码的提交，运行单元测试，反馈集成结果。
- 通过构建工具对代码进行编译、打包和部署。
- 通过版本控制工具实现版本控制与管理。

持续集成测试阶段的工作主要是，CI 系统根据集成结果进行不同操作，如果成功，则将代码合并到主分支；如果失败，则反馈给开发人员修改重新提交。

从中我们可以看到，持续集成涉及的主要工具类别包括：

- 版本控制工具——实现源代码管理、版本控制。
- 构建工具——实现代码的自动化编译、打包等，这是持续集成的核心工具。
- 测试工具——实现代码的自动化测试，以及大型测试或专项测试。
- CI 系统——整合版本控制、构建工作和测试工作，实现持续集成。

Python实践： gitlab + Sonar + flake8 

##### 分支管理

根据实际应用场景随时调整。如下，是一种：

只有 master 和 develop 两个分支是主干分支，一直存在的。其他分支都是功能性分支，在完成历史使命后会，可以被删除。

当使用master作为发布分支时，可以打上额外的tag便于版本回滚和追溯

```bash
# 合并master/devlop分支之后，打上tag 
# -a, --annotate        annotated tag, needs a message
$ git tag -a v0.1.0 master
```

master 分支为主分支，用于部署生产环境，需要确保master分支的稳定性。
此分支属于只读分支，只能从 release或hotfix 分支合并过来，任何时候都不能在此分支修改代码。
所有向master分支的推送，都要打上tag标签记录，方便追溯。
此分支只能前进，不能有回退操作。

#### TDD：测试驱动开发

Test Driven Development

代码覆盖率只是说明，代码实现者**能考虑到的那些场景**，已经测过了。

如果有一个场景代码实现者漏掉了，对不起，即使覆盖率 100% 也于事无补。

**因此，代码覆盖率是用来度量测试用例的，而不是用来度量代码的。**

##### 过度追求覆盖率

明白测试用例的作用之后，才会发现以前一味的追求代码覆盖率，真是**搬石头砸自己的脚**，
每加一点功能，都得**先写代码，然后补单测**，不然代码覆盖率要降低了，
改一些东西呢，又得先改代码，然后改单测，不然单测过不了。

##### DDD：场景驱动设计

写得不错：http://zhangyi.xyz/scenario-driven-design/

我对场景的定义为：**具有业务价值的，由参与者触发的，按照时序排列的一系列连续执行的任务过程。**场景的层次与Alistair Corkburn设定的用例层次一致，可以简单分为三个层次：概要目标、用户目标和子功能。

场景驱动设计的过程分为三个步骤：

1. 识别场景：从需求中识别出独立的具有业务价值的领域场景
2. 分解任务：根据职责的层次对领域场景进行任务分解
3. 分配职责：为领域驱动设计角色构造型分配不同层次的职责

分解任务其实最符合设计者思维方式，这其实是一种自顶向下的设计方式，它同时也作为测试驱动开发的前置条件。我根据子任务的粒度，将这些任务分为“组合任务”和“原子任务”。任务的类别划分直接影响到后面的职责分配。

在场景驱动设计中，发挥重要的角色构造型包括：应用服务、领域服务、聚合和网关。它们与场景及任务存在以下对应关系：

- 应用服务：场景，体现业务价值
- 领域服务：组合任务，封装多个领域对象之间的协作
- 聚合：代表领域行为的原子任务
- 网关：访问外部资源的原子任务

##### 真正测试驱动

那么 TDD（测试驱动开发）到底包含了哪些**思想**呢？
能给我们的编程实践带来哪些**启示**呢？

我认为至少包含两个。

**（1）场景驱动**
测试驱动的本质是场景驱动，
为了能够”**先写测试程序，然后编码实现其功能**“，我们必须提前考虑代码要支持哪些场景。

需要支持的场景，代码**必须实现**，
不需要支持的场景，代码**没必要过度设计**。

软件设计中遇到的很多问题，都可以用场景驱动的方式避免。
**TDD（测试驱动开发）直观的解释了，什么样的设计才叫”刚好够用“。**

**（2）自动化**
被自动化测试用例覆盖的代码，其**修改方式**是受保护的，
不恰当的修改，会导致单测不过，更早被发现。

这就是《[重构](https://link.zhihu.com/?target=https%3A//book.douban.com/subject/4262627/)》这本书的前提条件，

> [代码重构](https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E4%BB%A3%E7%A0%81%E9%87%8D%E6%9E%84)，指对软件代码做任何更动以增加可读性或者简化结构**而不影响输出结果**。

关键是后半句，”**而不影响输出结果**“，
怎样才能保证不影响输出结果呢？只能通过**测试**。
而写出**能自动化运行**的测试用例，会降低测试**成本**，在修改代码时也能得到更快的**反馈**。

很多人对”**重构**“有误解，以为重构只是**单纯调整代码结构**就完事了，
其实，这样的重构不叫”重构“，叫做**重写**。
是引发大量[线上故障](https://www.zhihu.com/search?q=线上故障&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A718337697})的根源。



作者：何幻
链接：https://www.zhihu.com/question/329784671/answer/718337697
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



#### 持续测试：质量门禁

质量门禁的前提是stage执行成功，质量是在成功的基础上进行的更严格的把控。

> 流水线在任意stage失败后，流水线就会终止执行。

持续测试是指开发人员提交代码后自动运行相关单元测试，给出测试结果的反馈；如果失败，则会在测试结果的详情页面中输出错误提示。

持续测试应当输出测试报告，报告是将持续集成的运行情况以适当的形式展现给相关人员的基本方式。报告是持续集成的晴雨表，所以它必须直观、易懂，比如开发人员从错误日志中找到代码出错的位置和原因；QA 测试人员发现测试的覆盖率和执行情况；管理人员从持续集成通过率的趋势中了解到项目的进度和质量。

测试用例--

测试标准建议

代码质量：sonar检查无Blocker和Critical级别问题。
单测覆盖率：代码行覆盖率和分支覆盖率不低于80%，进一步代码圈复杂度也可进行配置卡点。
接口自动化测试：接口通过率100%。
性能测试：通过率100%（TP99低于200ms，请求成功率大于99%）

https://stackoverflow.com/questions/4904096/whats-the-difference-between-unit-functional-acceptance-and-integration-test



##### 测试环境治理

https://cloud.tencent.com/developer/article/1506576

##### 测试用例: Test Case

测试用例对比 用户故事(User Store)

简单的，根据接口文档，写出如下：

接口：

输入：

期望：

健壮：故意输入错误数据

错误：哪些是可以忽略的Error.

测试执行的效率是指测试工程师每天执行的测试用例数，一般每天执行 50 条测试用例。

测试执行的质量包括两个方面：一方面是指每个测试用例发现的缺陷数；另一方面是指软件发 布后遗留的软件缺陷数占总缺陷数的百分比，一般要求低于 0.5%

##### 测试计划

测试计划对标发布计划。

##### 测试分层

越是底层的测试，牵扯到相关内容越少，而高层测试则涉及面更广。比如单元测试，它的关注点只有一个单元，而没有其它任何东西。所以，只要一个单元写好了，测试就是可以通过的；而集成测试则要把好几个单元组装到一起才能测试，测试通过的前提条件是，所有这些单元都写好了，这个周期就明显比单元测试要长；系统测试则要把整个系统的各个模块都连在一起，各种数据都准备好，才可能通过。



##### 单元测试： Unit Tests

Tests the smallest unit of functionality, typically a method/function (e.g. given a class with a particular state, calling x method on the class should cause y to happen). 

单元测试是保证你写的代码是你想要的结果的最有效办法。

单元测试假定只要所有函数都正常工作，那么整个产品就能正常工作。

单元测试讲究的是两个指标：

* 单元测试覆盖率：超过80%， 测试覆盖率是指在测试过程中对被测试对象的需求、功能、代码测试的程度
* 单元测试通过率：100%，不通过说明函数功能有问题呢

https://jiakui.app/2019/08/22/unit-test/

单元测试是所有测试中最底层的一类测试，是第一个环节，也是最重要的一个环节，是唯一一次有保证能够代码覆盖率达到100%的测试，是整个软件测试过程的基础和前提，单元测试防止了开发的后期因bug过多而失控，单元测试的性价比是最好的。

##### 冒烟测试：Smoke Testing

冒烟测试的目的：确认提测模块的基本功能、实现逻辑符合需求，可以进行后续的正式测试工作，将正式测试未知的风险降至最低，防止bug阻塞导致测试进度阻塞 。

冒烟测试就是在每日build（构建版本）建立后，对系统的基本功能进行简单的测试。这种测试强调程序的主要功能进行的验证，也叫版本验证测试，提交测试。

冒烟测试, 是指在对一个新版本进行系统大规模的测试之前，先验证一下软件的基本功能是否实现，是否具备可测性。所以也叫可测性测试。

##### 端到端测试E2E：就是联调--

端到端测试 End-to-End Testing：在真实场景中从头到尾执行测试整个应用程序 (类似生产场景），测试应用程序流是否按照设计的方法，例如与数据库、网络、硬件和其他应用程序通信。从用户角度执行测试，关注用户使用场景。也称为链测试（Chain Testing）。 也可以简单粗暴的理解为我们日常所说的联调测试。

端到端测试目的：识别系统之间的依赖关系，并确保在不同的系统组件和系统 (如使用网络通道的数据库或 GUI) 之间传递正确的信息。
端到端测试范围：1) 验证被测软件系统 2) 检查被测系统与外部接口的集成,验证来自上游/下游系统的批处理/数据处理

##### 回归测试：

回归测试一般用于软件后期迭代和维护过程中。主要是因为可能新 Bug的发现，修复后对其它功能造成的影响，或者开发者对错误的理解不够透彻，引发的其它 bug。新功能的追加，新代码可能对原有代码带来的影响。

回归测试主要是杜绝新代码的融入给原版本带来的不可预测的影响或者引发其他bug 的产生。

回归测试，是指对软件的新版本测试时，重复执行之前某一个重要版本的所有测试用例
目的：
验证之前版本产生的所有缺陷已全部被修复；
确认修复这些缺陷没有引发新的缺陷

##### 系统测试:system test

系统测试是在系统未交付使用制作方自行进行的测试，无用户参与，只有本方用户参与。

ST更关注系统中每个点的测试，单纯地从系统层面出发，比如很纯粹的接口测试，很纯粹的页面检查，很纯粹的溢出检查，不会涉及到具体的业务流.

##### 拨测：包含网络链路

总结：拨测是一种网络链路质量的测试手段。拨测，非常类似于爬虫，更准确地讲，非常类似于黑客控制“肉鸡”发起[DDos攻击](https://so.csdn.net/so/search?q=DDos攻击&spm=1001.2101.3001.7020)。这里的“肉鸡”，就是某个互联网服务的客户端，比如PC端、手机端。

目的：探测各地区用户到各个服务接入点的链路状况，这样，服务调度系统就可以根据探测结果为用户提供最佳的接入点。

注意：拨测会占用网络资源，应该选择在空闲时间段进行；拨测也会消耗“肉鸡”的网络流量，不过一般有限制，比如手机端限制在几百KB，以免被用户发现。



##### UAT（用户验收测试）

UAT(User Acceptance Test)：是本方人员和用户方一起实行的测试。 要是深入吃透这两点，你就知道这两点的差别就是是否有用户参与。

UAT从最开始的案例编写就应该从整个业务流的角度出发，从系统使用的实际场景出发，而无需去关注一些业务上不可能发生的问题（但这种问题ST却要关注，例如银行转账系统，ST测试发现转账时，当用户的姓名大于100个字符时，转账会报错……但UAT可能就会放过这样的“缺陷”，因为现实中不可能出现用户姓名大于100个字符的情况吧！）
然后在测试方法上，我认为UAT的测试策略就应该是全黑盒的，例如，用的非常多的一种测试方法就应该是“场景法”；而ST的测试策略应该是“灰盒”的才对。
其实在实际的工作中，ST和UAT难免会在某种程度上有交集，但我认为只要关注好不同测试的关注重点，这两种测试还是有很大区别的，都是保证软件质量必不可少的重要环节。

##### 如何实现持续测试： Coding

持续测试改变的是传统测试后置的工作模式，让测试活动延伸到软件开发生命周期的每个阶段。

**1）需求规划阶段，尽早计划测试，并且策略性定义测试子集**
首先，从需求分析的阶段就开始提前计划测试、编写测试用例，使之达成适当的需求覆盖率。对此有帮助的实践包括 ATDD、BDD，尤其是 TDD 难以落地的团队可以尝试 ATDD。
其次，要有优化测试覆盖范围的意识。测试不应该盲目追求 100% 覆盖，而是基于业务风险和价值的测试策略进行测试（Risk-based Testing），“100% 覆盖优先级高的需求”远比“80% 覆盖了所有需求”来得有价值。

**2）迭代进行当中，推动测试左移（Shift-left），实现测试与开发并行工作**
测试执行应该前置到软件开发生命周期的早期，多种工程实践可以帮助团队实现左移：比如重视测试评审，通过单元测试进行基础性保障，基于接口定义的开发和自动化测试，引入代码扫描判断是否满足编码规范和工程标准。这样在迭代周期内，围绕着需求持续进行集成测试用例的编写，并且与开发保持进展协同，为开发提供必要的测试支持，使得测试与开发的工作实现同步进行。

**3）迭代进行当中，以便捷的方式提供完整的测试环境和正确的测试数据**
一直以来，接近生产的测试环境打造和脱敏数据的快速准备是团队面临的两大重要挑战。现今随着云原生技术的成熟，尤其是 Docker 技术让按需搭建和销毁环境变得可能。但是测试数据的管理仍然是个难题，基础数据像账号信息、环境信息这一类容易标准化的数据在业内已经有了比较好的解决方案，这已经是个重大进步。而业务数据由于场景多变性一直缺乏足够好的抽象，还处于依赖框架进行流程规范的基础阶段，基于接口定义的开发从而实现 Mock 服务也能够带来过程效率的提升。

**4）应用部署之后，关注测试右移（Shift-right）**
传统瀑布模式把部署作为测试的下一阶段，也就意味着应用发布上线、快速验证功能之后就是测试的结束。而持续测试则不认为发布完成测试就退出了，强调的是在版本上线后、继续关注生产环境的数据监控和预警，及时发现问题并跟进解决，将影响范围降到最低。并且利用生产上的数据可以为开发过程带来切实的价值：比如复制生产数据进行脱敏来准备测试数据，对服务访问数据进行分析的结果也可为开发过程中的测试提供优化的指引、从而调整测试并形成更好的冒烟和回归测试策略等等。右移的实践包括数据分析、灰度/金丝雀发布、线上实时监控、用户反馈的跟踪处理流程等等。

此外，我们在实践持续测试的过程中要关注数据的沉淀，然后基于数据指标不断优化我们的行为，从而实现 DevOps 所推崇的持续改进的团队文化。

##### newman

　　Newman 是 Postman 推出的一个 nodejs 库，直接来说就是 Postman 的json文件可以在命令行执行的插件。
　　Newman 可以方便地运行和测试集合，并用之构造接口自动化测试和持续集成

Newman is a command-line collection runner for Postman. It allows you to effortlessly run and test a Postman collection directly from the command-line. It is built with extensibility in mind so that you can easily integrate it with your continuous integration servers and build systems.

Newman是Postman的命令行收集运行程序。它使您可以直接从命令行轻松地运行和测试Postman集合。它在构建时考虑了可扩展性，因此您可以轻松地将其与持续集成服务器集成并构建系统。

```bash
newman run --reporters cli,json  --insecure -e /home/auto_test_plan/2/1469115219069370368/execute-environment.json --reporter-json-export /home/auto_test_plan/2/1469115219069370368/execute-result.json  /home/auto_test_plan/2/1469115219069370368/execute-collections.json > /home/auto_test_plan/2/1469115219069370368/execute-log.log
```

end

##### Jemeter

总结：拨测是一种网络链路质量的测试手段。拨测，非常类似于爬虫，更准确地讲，非常类似于黑客控制“肉鸡”发起[DDos攻击](https://so.csdn.net/so/search?q=DDos攻击&spm=1001.2101.3001.7020)。这里的“肉鸡”，就是某个互联网服务的客户端，比如PC端、手机端。

目的：探测各地区用户到各个服务接入点的链路状况，这样，服务调度系统就可以根据探测结果为用户提供最佳的接入点。

注意：拨测会占用网络资源，应该选择在空闲时间段进行；拨测也会消耗“肉鸡”的网络流量，不过一般有限制，比如手机端限制在几百KB，以免被用户发现。

##### 坡度 and 平行测试

指测试过程中测试压力变化过程：

* 坡度：测试压力逐渐增加
* 平行：以固定压力持续测试

##### 测试挡板

可以出口挡板定义访问应用某个类的方法时，直接返回mock的数据。

##### 测试影子库

压测时使用额外的表和库/缓存key，在生产环境模拟用户真实场景的情况下保持生产库的干净，不混入测试数据。

影子库和表通过脚本清理。

#### 持续交付： Delivery，制品库

https://help.coding.net/docs/artifacts/intro.html

实践，并不是每个分支都需要到交付阶段的，开发在自己的分支开发完成，只需要自己的测试就行了，不需要通知QA进行验证，只有要发布的版本才需要QA介于即提测阶段

持续交付和持续部署是两个非常容易混淆的概念。持续交付指的是频繁地将软件的新版本交付给 QA 测试团队或者运营团队，如果评审通过，代码就进入发布、生产阶段。

产品持续交付的角色应由 QA 测试人员来担任，开发人员也需要参与到产品的某一轮系统测试中，并随时准备 Bug 审查和 Bug 修复。最后，由 QA 测试人员根据具体的产品缺陷、质量情况等做出产品是否适合面向用户交付、发布的决定。

交付，是指经过QA检测可以用于发布的版本。

#### 持续部署：Deployment

这里需要运维和测试的赋能，将流程自动化和工具化。

所有通过了测试验证的软件都自动部署到生产环境中。

#### 可观测性：666配合压测

观测云的可观测平台和数列的全链路压测平台都挺不错哈

压测是暂时且数据孤立的，压测时通过可观测平台（log、metirc和trace）分析压测时，系统多维度的指标数据，才能更好的定位问题和发现系统性能瓶颈，从而达到压测的最佳价值。

从三个大方面入手，各有很多开源产品。

* Logs：
* Metrics：
* Traces：

可以搞一个整个的产品出来，作为卖点。

#### 每日构建和每日测试

每日构建（Daily Build）在反应速度上没有持续集成快，它更强调的是通过每天（通常是晚上）自动部署当天开发所累积的代码，并结合每日测试（Daily Test）方法进行自动化测试，用于评估和衡量项目的进度。

持续集成特性决定了不可能在该阶段加入大量的测试，而每日构建则一般是在夜里执行的，那么这时就可以做更多的事情，比如代码质量检查、单元测试、测试覆盖率、集成测试等。每日构建会把当天所有的提交代码都一起做集成，而不像持续集成那样每提交一次代码就做一次集成。持续集成属于增量式构建，而每日构建则是一次完全构建。

每日构建是项目的心跳线。一般而言，通过每日构建我们可以看到项目的进度。比如今天有10个 Bug 的修复代码都提交了，晚上进行自动部署并通过了自动化测试，第二天上班时通过查看邮件我们就可以发现每日构建成功了，那么项目就又往前迈了一大步，开发工作又有了具体的进展。

具体的，每日构建就是在每天晚上自动化部署一个 OpenStack 环境（可以是all-in-one，也可以是 multi-node），然后运行大型或专项测试，比如 API 性能测试、API 接口测试、功能测试、部署测试等。

#### Sonar/Coverage工具

代码覆盖率：检测测试代码的有效性，即测试case对被测代码的覆盖率几何。


--没玩过 好像很厉害

Sonar是从七个维度检测代码质量，而作为开发人员至少需要处理前5中代码质量问题。

1、不遵循代码标准

  sonar可以通过PMD，CheckStyle，Findbugs等代码规则检测工具**规范代码编写**

2、潜在的缺陷

  sonar可以通过PMD，CheckStyle，Findbugs等代码规则检测工具**检测出潜在的缺陷** 

3、糟糕的复杂度分布

  文件、类、方法等，如果复杂度过高将难以改变，这会使得开发人员难以理解它们，且没有自动化的单元测试，对于程序中的任何组件的改变都将可能导致需要全面的回归测试

4、重复

  显然程序中包含大量复制粘贴的代码是质量低下的，sonar可以展示源码中重复严重的地方

5、注释不足或者过多

  没有注释将使代码可读性变差，特别是当不可避免出现人员变动时，程序的可读性大幅度下降，而过多的注释又会使得开发人员将奖励过多的花费在阅读注释上，亦违背初衷

6、缺乏单元测试

   sonar可以很方便地统计并展示单元测试覆盖率

7、糟糕的设计

  通过sonar可以找出循环，展示包与包、类与类之间相互依赖关系，可以检测自定义的架构规则 通过sonar可以管理第三方的jar包，可以利用LCOM4检测单个任务规则的应用情况，检测耦合。

 PMD，CheckStyle，Findbugs这些工具都叫**静态代码分析工具**。何为静态代码分析？静态代码分析是指**无需运行被测代码**，仅通过分析或检查源程序的语法、结构、接口等来检查程序的正确性，找出代码隐藏的错误或缺陷，如参数不匹配，有歧义的嵌套语句，错误的递归，非法计算，空指针引用等。

#### 不同环境

pro（Production environment）：生产环境，面向外部用户的环境，正式环境，连接上互联网即可访问。

sit(System Integration Test ): 系统集成测试，开发人员自己测试流程是否走通。

uat(User Acceptance Test environment): 用户验收测试环境，用于生产环境下的软件测试者测试使用。

test: 测试环境，外部用户无法访问，专门给测试人员使用的，版本相对稳定。

pre ：灰度环境，外部用户可以访问，但是服务器配置相对低，其它和生产一样，外部用户可以访问，版本发布初期，正式版本发布前。

dev （Development environment） ： 开发环境，外部用户无法访问，开发人员使用，版本变动很大。

fat (Feature Acceptance Test environment) : 功能验收测试环境，用于软件测试者测试使用

#### 总结：

Continuous Integration ： 经常合并，并且通过单元测试和代码规范（flaake8、Alibaba Java Coding等）

这些静态代码分析工具很多可以在IDE上以插件形式存在，可以开发阶段提交代码之前，开发人员自测。

Continuous Delivery:  QA介入执行复杂的测试：回归测试和集成测试（冒烟测试）等等。

Continuous Deployment：业务负责人或开发人员或产品经理，通过部署工具或界面实现自动化部署到生产环境。

在没有制定出工作流程时，便考虑选择持续交付工具集，是最不重要的决策。无论是 CI 还是 CT、CD 等，都可以使用众多的开源工具，这样做没有被锁定的风险，可以自由切换等。

#### 实践：看-

DevOps目的是为了减少重复的工作，而不是减少必要的流程。

一个内部的CI/CD工具，它包含：

* 打包编译成果、构建镜像
* 通知信息到提测中心，用于通知开始持续交付，QA测试完成没有问题后，在通知运维平台，用于持续部署。
* 通知信息到运维平台，用于通知开始持续部署

使用gitlab gitlab-ci，根据分支管理策略，定义CI/CD流程：

* 非主干分支（hotfix、feature等）：触发流程：CI --> Continuous Deploymemt，代码自动测试成功后，直接部署到相应非正式环境。
* 主干分支（用于发布的分支）：触发流程：CI --> Continuous Delivery，QA介入测试没有问题后，Continuous Delivery --> Continuous Deployment

结合其他人工流程：

* 代码合并前，必须给出相应的测试报告，批准后可以合并或者必须代码被review后才运行提交

* 代码测试，QA测试完成要发布的版本后，该版本才会出现在发布选择列表用。

  代码质量：sonar检查无Blocker和Critical级别问题。
  单测覆盖率：代码行覆盖率和分支覆盖率不低于80%，进一步代码圈复杂度也可进行配置卡点。
  接口自动化测试：接口通过率100%。
  性能测试：通过率100%（TP99低于200ms，请求成功率大于99%）

* 代码部署，业务负责人/项目负责人/运营，定义发布窗口和发布执行人，执行发步和验证发布结果。

比如自下而上流程：先指定版本再测试

* 申请发布窗口，指定发布版本
* QA测试发布版本
* 指定人员，执行发布任务

自下而上也行啊：只有测试通过的版本才能被看到。

* 开发人员合并代码到发布分支或tag
* QA收到提测要求，测试ok，则将新的发布信息同步到发布平台
* 创建发布窗口，选择可以发布的版本
* 执行发布

### Jenkins

#### Job

一个job执行特定的构建、编译或部署任务，可以被其他pipeline或job引用。

Name of a downstream job to build. May be another Pipeline job, but more commonly a freestyle or other project. 

#### Pipeline

## 专家：

首先，你要相信自己是无敌的，在这块领域--

### 设计DevOps流程

甲方（湖北中烟）的DevOps想法：

1. 代码一致：

2. isv操作开发分支和测试环境流水线; 

3. 中烟运维团队负责合并merge和生产流水线创建。
   (开发) ||(测试和生产)隔离
   开发人员只能在开发环境随意构建和操作。

   测试和生产流水线，由专业运维团队来操作。

4. 测试环境和生产环境，需要通过制品流转来保证发布质量。

   即要定制镜像从测试到生产的策略。网易：通过特殊tag匹配，其他使用`x.y.z`版本号，`z`为偶数时，自动同步到生产库。

开发人员要保证的就是 代码 没问题！！！ 其他他们不用管了
这种玩法的危险点：
1， 构建任务参数不一致
2， 基础镜像不一致

但是作为devops专家，要点出来和给予解决办法。

### 落地：看

阶段一：流程不变（尽量先保存原流程不变只提高自动化能力，便于大家接受DevOps），只是去掉人工参与工作。操作对象为isv/研发人员，运维人员负责审核。

这个阶段主要是，使用DevOps工具链去除人为重复工作以及工具稳定性的验证（最终目的是任何人都可以使用工具实现稳定的发布）

阶段二：优化流程，精益求精。

这个阶段的前提时，DevOps工具链已经十分稳定，发布流程已经工具化和自动化，这时开始优化流程，可以是不懂技术的业务负责人或者运营人员直接发布。

彻底完成赋能，并且定制流程--

核心：

* 先根据现有发布流程，只做自动化优化，用一段时间后，客户或者公司有了自己的想法和流程，再重新定义DevOps流程
* 定义属于自己的公司或者满足客户需求的DevOps流水线

end

### User Story and Task

User Story： 用户关心的需求,可以展示的

Task：开发根据User Story 拆分的实现步骤，不用展示。



### 华为敏捷资料

https://support.huaweicloud.com/reference-devcloud/devcloud_reference_030101.html

### devops： 开发视角

代码变更： 可追溯 和 缺陷管理对应。即每次commit和一个Tapd 需求或者task关联，便于追溯

CLoudOS集成了Sonar 会出扫描报告。

基于postman做测试用例，方便开发人员导入和管理。 postman工具 Newman？？？

