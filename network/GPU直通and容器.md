### GPU直通and容器

### GPU直通

https://mechanical-consciousness.com/2020/03/20/kvm-gpu-passthrough.html

GPU 直通是指服务器的GPU以直通方式分配给虚拟机，并通过远程协议使得用户可以从远程进行接入。



GPU直通方式，将承载虚拟机的云平台的物理节点上的GPU显卡绑定分配给用户虚拟机，终端用户通过终端远程接入用户虚拟机，这样虚拟机就可以使用GPU获取3D加速能力。

优势在于：支持多种类型的显卡，兼容性好，支持符合最新DirectX、OpenGL规范的3D应用。



整个虚拟机GPU直通大约有那么几个步骤（缺1不可），我的平台是Intel + Nvidia，因此用AMD的朋友可能会有所不同。但是整体思路应该是一致的。

- 确认硬件支持

  主要是CPU和主板需要支持VT-d技术，如果是AMD平台的话，应该是AMD-Vi支持。 具体行不行，直接Google之。

- 买买买

  如果你只有一张显卡的话，GPU直通显然是没法玩的，显卡分配给了虚拟机，宿主机怎么办？

  当然你可以搞两张一样的卡，我个人的话直接搞了张二手GTX 650，便宜实惠。反正宿主机对显卡也没啥要求。

  理论上来说A卡N卡混合应该无所谓，但我没试过。

  理论上来说，如果你有集成显卡，有些主板有选项，可以默认从集成显卡启动，这样的话你可以省掉一张显卡。（很遗憾i9连核显都没有） 省掉一张显卡主要不是为了省钱啊，我现在的机箱已经重到搬不动了，两张显卡，3块大硬盘，2个SSD，还有水冷啥的，塞的满满当当的，囧。

  另外推荐入手一个kvm切换器，等显卡直通搞定了，你就相当与有了两台电脑了，没个kvm不得烦死？

  另外，如果主板的所有USB口都在一个iommu group的话，推荐再入手个PCI-E的usb扩展卡。后面会仔细解释这个事情。

- 在BIOS中调整相关参数

  首先肯定是要启用VT-d的啦，（AMD的话就是AMD-Vi）

  然后还有一个重要选项是默认从哪张显卡输出。可惜不是所有BIOS都支持这个选项，我的老板子就不支持，前端时间坏了（真假），被我顺利淘汰了。

  如果有这个选项的话，就可以把好显卡插在主PCI-E槽，然后默认从差的显卡启动。如果没有的话，想开点，交换一下吧。

- 在宿主系统中启用iommu

  iommu是GPU虚拟机直通的核心。

  虽然我们总是在讲GPU直通，其实本质上来说，通过VT-d技术，各种乱七八糟的东西都可以丢到虚拟机里面去，让虚拟机直接访问。

  iommu负责把主板上的设备分组，分成一个一个group。每个group都可以单独的启用或者停用。

  在没启用iommu之前，宿主系统会占用全部的硬件。在启用iommu之后，宿主系统可以有选择的预留一些硬件资源，把这些硬件资源分配给虚拟机。

- 在宿主系统中预留iommu group

  无论有没有启用iommu，Linux默认会扫描系统中的所有硬件，并尝试给每个硬件分配驱动，并初始化。

  这个事情的直接后果就是，系统启动完了后，iommu倒是启用了，但是没有闲着的分组？？？

  所以我们需要在Linux内核启动足够早的阶段把硬件预留下来。

  这个步骤是整个事情中最麻烦的，也最有可能遇到问题的。

- 创建kvm虚拟机，并将iommu group分配给虚拟机

  这个简单

- 给虚拟机安装系统

  emmm….

- 阻止nvidia驱动发现虚拟机

  这个步骤我是真的真的想吐槽，所有一切都就绪了之后，系统起来被锁定在800x600的分辨率，驱动提示错误35。

  刚开始我始终以为是自己哪里做错了。结果上网一搜发现，竟然是nvidia的驱动直接去侦测自己是不是在虚拟机里跑的。如果是的话就直接报错。

  修正这个问题不难，kvm自身有特性可以隐藏虚拟机，直接打开就能解决这个问题。但是这破问题坑了我好几个小时。

### 容器使用GPU

步骤：

1. 物理机加上gpu
2. 虚拟机配置gpu直通
3. 容器使用gpu

但是gpu直通方式，只能将gpu绑在一个物理机上，vm不能迁移。

使用vGPU就很灵活，但是英伟达要售授权费...

### vGPU

AI落地时，在某些场景下AI模型在训练或者是推理时，其算力要求不需要占用整卡的GPU，比如只需要0.5卡GPU即可满足需求。在这种情况下，可以使用GPU虚拟化技术来解决这个问题，将整卡的GPU虚拟化为两个0.5卡的GPU，这样就可以在一张卡上同时跑两个AI训练或者AI推理应用服务，极大压榨算力资源，降低成本。
基本痛点：

- 容器的GPU利用率不够高，特别是推理任务；
- 为了提高GPU的利用率、避免算力浪费，需要在单个GPU上运行多个容器；

目前常见的NVIDIA GPU虚拟化技术方案有：NVIDIA GRID(vGPU)，NVIDIA MPS，cGPU和vCUDA。其中，在k8s容器上常用的为vCUDA和cGPU方案：

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/gpu虚拟化.jpg)

NVIDIA提供的两种方案还有一个缺点就是其算力共享是平均共享的，无法按照一定权重分配共享，比如A应用主要0.1算力而b应用分0.9算力。

### cGPU

> 这里说的cGPU为阿里云异构计算cGPU容器技术架构

创新的基于内核虚拟GPU隔离的GPU共享容器方案，实现了用户无感且轻量的GPU虚拟化技术。使用标准的Docker，可以无缝的兼容Kubernetes，做到AI应用无需做任何修改或重编译，可直接运行。物理GPU的资源可以任务划分，包括显存和算力 (实际使用经验，建议以间隔显存大于1G为单位做切分)。



其本身是支持几种调度策略，这里常用的为基于权重的抢占调度：
cGPU服务按照容器数量（max_inst）将物理GPU算力划分成max_inst份，假如有两个docker容器共享一个GPU，设置示例如下：

- Docker1的权重为m
- Docker2的权重为n

则调度效果如下：

- 如果只有Docker1运行，则Docker1会抢占整个物理GPU的算力
- 如果Docker1和Docker2同时运行，则Docker1和Docker2获得的理论算力比例是m:n

按照算力的时间片轮转，根据比例赋予不同Docker容器进行计算占用时间

### vCUDA

> 这里说的vCUDA是腾讯TKE团队的开源版本

vCUDA的系统架构与NVIDIA的GRID架构类似，采用一个Manager来管理GPU，Manager负责配置容器的GPU计算能力和显存资源，做到使用者无法使用多余申请的显存，GPU的平均使用率不会大幅超出申请值。vCUDA的设计只侵入了CUDA层，用户的程序无需重新编译就可以运行在基于vCUDA的GPU实现共享。vCUDA使用修改后cuda library来达到资源控制，vCUDA分别修改了计算操作，显存操作和信息获取3个方面的API。



### h3c

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/gpu-vm.jpg)



### 引用：

1. https://mechanical-consciousness.com/2020/03/20/kvm-gpu-passthrough.html
1. https://zhuanlan.zhihu.com/p/268901942