## 组网架构

### 网络先行:first

任何大平台的规划，都得从网络架构开始。

网络分层分区规划好了，然后忘各个区域增加物理设备充当资源池，最后才是应用层面的高可用落地。





### 问题：

同一个VPC 内 同一子网不同 AZ 二层互通问题：

AZ_A网络： 1.1.1.0/24， gw： 1.1.1.1

AZ_B网络： 1.1.1.0/24     gw:    1.1.1.1

二层互通，因为A和B两个网络都有网关，独立AZ有独立的出口IP。

这时，跨AZ互通时，网关在哪个AZ部署，两个都是独立网关肯定不行的，路由不好做。

实现方式：

1. 两个网关，但是是主备模式，只有一个负责转发。

### 租管互通:fried_egg:

配置某些节点的管理网和业务网段互通，实现在plat集群上下发vm或其他云产品到不同的计算可用域--

#### 管理网 vs. 业务网

因为管理网权限大，所以不希望租管互通。

我一直以为是不能租管互通，是怕管理网访问业务网... 原来是我完全想错了...

**租管互通的风险是业务网被入侵后可以到达全部的管理网。**

##### 实际场景

管理交换机和业务交换机是两个不同的交换机，已知虚拟资源池(虚拟化平台)的物理机都有管理网卡和业务网卡，现在需要通过管理网下发一批只有业务网的虚拟机，该如何实现？

> 管理和业务交换机通过另一台核心设备才能配置互通。

* 在上层设备配置互通：在核心(三层)或者Border switch加防火墙上打通管理和业务网络。

* 无需上层设备配置互通：在业务网段资源池的虚机上在创建一个网卡，绑定一个私有网段即可，然后配置这个管理网段和私有网段互通（配置静态路由）

  相当于给要创建的业务网段主机，植入一个管理网可以联调的私网网段而且不用借助上层设备转发，在虚拟化平台层面就可以实现互通了





### 网关不要放在防火墙上

漏扫或者风暴，流量容易打挂防火墙！！！

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/gateway-and-firewall.jpg)



不过，还好的是，这里只是带外网关挂在防火墙上了，流量小。



### Region and AZ

Region，不同城不同机房

**Available Zone**，同城不同机房

Region和AZ怎么区分，判断标准就是 延时。

腾讯的标准

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/az-and-region.png)

H3C的标准

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/az-and-region-h3c.png)

同城双中心：One Region，Multiple Available Zone

两地双中心：Multiple Region， One Zone

两地三中心：Multiple Region，Multiple Available Zone

### 传统三层网络架构

传统三层架构网络的高可用（冗余设备、冗余链路），为了避免多链路造成的广播风暴，就必须使用X.STP等破环协议。由于STP的性能限制，一般不能超过50个网络节点，导致虚拟机受困在较小的局部范围，服务应用均收到限制。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/三层网络架构.png)

### 二层技术演变

http://lms.h3c.com/zxy-student-web/#resource/course/player-nt/2c998c94730f08d10177b7ec04ec1693/2c998c94730f08d10177fc5fabf25961

STP（消除L2环路） --》 IRF（利用全链路） --》TRILL --》 Overlay

　　由于STP的收敛性能等原因，一般情况下STP的网络规模不会超过100台交换机。同时由于STP需要阻塞掉冗余设备和链路，也降低了网络资源的带宽利用率

### Switch Fabric：概念，不是实现

**Clos、Spine-leaf、Trill都是Switch Fabric的一种。**

自从 1876 年电话被发明之后，电话交换网络历经了人工交换机、步进制交换机、纵横制交换机等多个阶段。20 世纪 50 年代，纵横制交换机处于鼎盛时期，纵横交换机的核心，是纵横连接器。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/switch-fabric-1.png)

因为开关矩阵很像一块布的纤维，所以交换机的内部架构被称为 Switch Fabric（纤维），这是 Fabric 成为计算机网络专业术语的起源。

Switching fabric is a network topology wherein different network nodes or terminals connect with each other via a number of switches, usually crossbar switches. 

Switching fabric may also be known as switched fabric or simply fabric.

Switching fabric is a combination of hardware and software that controls traffic to and from a network node with the use of multiple switches. Data comes in one port and out on another port. It is independent of the router and the bus infrastructure technology that is being used to move data between nodes. It usually makes use of shared memory and data buffers. This topology is quite complex and that is reflected by its name, which is likened to a fabric, belying a complex grid of connections, switching paths and nodes present in the network. The term is also loosely used to collectively refer to all switching hardware and software in the network.

#### Crossbar 架构

最简单的 Switch Fabric 架构是 Crossbar 纵横模型，这是一个开关矩阵，每一个 Crosspoint（交点）都是一个开关，交换机通过控制开关来完成输入到特定输出的转发。一个 Crossbar 模型如下所示。

N个输入和N个输出，相互通信，则每条线都需要有交点即N*N。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/switch-fabric-2.png)

《类似围棋棋盘的图》

> 可以脑补手动接线的场景吗，到了Switch Fabric架构，纵横交错的线实现互通。

随着电话用户数量急剧增加，网络规模快速扩大，交换机的端口数量逐渐增多。基于 Crossbar 模型的交换机的开关密度，随着交换机端口数量 N 呈 O(N^2) 增长。相应的功耗，尺寸，成本也急剧增长。在高密度端口的交换机上，继续采用 Crossbar 模型性价比越来越低。

**1. Crossbar架构**

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/crossbar-1.png)

可以理解为8口交换机，4口进，4口出。Input线和Output线的交叉的节点（CrossPoint）是开关。那么我们要建立1-2口的session，就要开启input1线和output2线的开关。同理session（2,4）（3,1）（4,3）；

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/crossbar-2.jpg)

Crossbar还有一个特点，就是当进行MxN交换时（即M个输入，N个输出）时，是有阻塞的交换，例如（3,2）的通路就占用了（4,1）的，所以（4,3)被阻塞了，不能同时转发。



总结一下，

a. Crossbar的优点是：

​    1.结构简单；

​    2.控制简单；

​    3.NXN交换时严格无阻塞；

b. Crossbar的不足是：

​    1.部署规模有限，CrossPoint的数量是N*N；

​    2.Input到Output只有一条通路，没有冗余；

​    3.这种简单结构不利于做大规模集成电路（VLSI），要做大容量就要叠加足够多的Crossbar；

​    4.输入输出不灵活，只能做NxN交换机，如果是要MxN交换机就成了有阻塞的交换

上一张1960年左右，Crossbar交换结构图，大家感受一下，

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/crossbar-3.jpg)

#### Clos : 架构

CLOS 交换模型的核心思想是：用多个小规模、低成本的单元，构建复杂、大规模的网络。简单的 CLOS 网络是一个三级互连架构，包含了输入级，中间级，输出级。下图中的矩形都是规模小得多的转发单元，相应的成本也很低。简单来说，CLOS 就是一种多级交换架构，**目的是为了在输入输出增长的情况下尽可能减少中间的交叉点数。**

> Clos 是为了改善Switch Fabric架构的中间交叉点数量。

现在流行的Clos网络架构是一个二层的spine/leaf架构，如下图所示。spine交换机之间或者leaf交换机之间不需要链接同步数据（不像三层网络架构中的汇聚层交换机之间需要同步数据）。每个leaf交换机的上行链路数等于spine交换机数量，同样的每个spine交换机的下行链路数等于leaf交换机的数量。可以这样说，spine交换机和leaf交换机之间是以full-mesh方式连接。

Clos架构，诞生于1952年，是由一位叫Charles Clos的人提出的，所以它并不是一个新的概念。

这个架构主要描述了一种**多级电路交换网络**的结构。Clos最大的优点就是对Crossbar结构的改进，通过Clos架构可以提供**无阻塞的网络**。

**下面，我们再来看看CLOS架构**

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/clos-1.png)



这个架构明显具有优势；

\1. 在大规模输入输出时，Crosspoint数量少。假设在NxN模式下，具体算法看下图

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/clos-2.jpg)

所以，我们计算总共N=20个输入，stage1上sub-switch上承担n=10个输入，stage2采用k=3个[sub-switch](https://www.zhihu.com/search?q=sub-switch&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A114944955})的情况，我们共需要crosspoint为136个，而采用Clossbar需要N*N，即400个crosspoint。



\2. 每个Session有冗余链路；

\3. 任何输入都能找到没有在同时使用的线路，所以叫做无阻塞架构（当然，也分情况，Clos在有些情况下也不是完全无阻塞）



**总结一下，CLOS的好处就是既节约的成本又增加了效率**。



4.CLOS的应用主要有两个方面，一个是交换机内部，另一个是[网络架构](https://www.zhihu.com/search?q=网络架构&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A114944955})。

a.交换机基本架构如下图，可以看到在交换机内部有个大大的switch fabric，它连接着input和output，所以在这个里面可以用到Crossbar或CLOS架构。例如Cisco [catalyst6500系列](https://www.zhihu.com/search?q=catalyst6500系列&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A114944955})交换机就是Crossbar交换机，Juniper EX9208就是CLOS架构

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/clos-3.png)

b.网络架构

例如facebook的网络架构，就采用的是IP CLOS

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/clos-4.jpg)

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/clos-5.jpg)

####  Spine-Leaf 架构

**在 Spine-and-Leaf 架构中，任意一个服务器到另一个服务器的连接，都会经过相同数量的设备（除非这两个服务器在同一 leaf 下面），这保证了延迟是可预测的**，因为一个包 只需要经过一个 spine 和另一个 leaf 就可以到达目的端。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/spine-leaf.png)

* Leaf Switch：相当于传统三层架构中的接入交换机，作为 TOR（Top Of Rack）直接连接物理服务器。与接入交换机的区别在于 L2/L3 网络的分界点现在在 Leaf 交换机上了。Leaf 交换机之上是三层网络，Leaf 交换机之下都是个独立的 L2 广播域，这就解决了大二层网络的 BUM 问题。如果说两个 Leaf 交换机下的服务器需要通讯，需要通过 L3 路由，经由 Spine 交换机进行转发。
* Spine Switch：相当于核心交换机。Spine 和 Leaf 交换机之间通过 ECMP（Equal Cost Multi Path）动态选择多条路径。区别在于，Spine 交换机现在只是为 Leaf 交换机提供一个弹性的 L3 路由网络，数据中心的南北流量可以不用直接从 Spine 交换机发出，一般来说，南北流量可以从与 Leaf 交换机并行的交换机（edge switch）再接到 WAN router 出去。

这种架构**每个低层级的交换机（leaf）都会连接到每个高层级的交换机 （spine），形成一个 full-mesh 拓扑**。leaf 层由接入交换机组成，用于连接服务器等 设备。spine 层是网络的骨干（backbone），负责将所有的 leaf 连接起来。 fabric 中的每个 leaf 都会连接到每个 spine，如果一个 spine 挂了，数据中心的吞吐性 能只会有轻微的下降（slightly degrade）。

如果某个链路被打满了，扩容过程也很直接：添加一个 spine 交换机就可以扩展每个 leaf 的上行链路，增大了 leaf 和 spine 之间的带宽，缓解了链路被打爆的问题。如果接入层 的端口数量成为了瓶颈，那就直接添加一个新的 leaf，然后将其连接到每个 spine 并做相 应的配置即可。这种易于扩展（ease of expansion）的特性优化了 IT 部门扩展网络的过程。**leaf 层的接入端口和上行链路都没有瓶颈时，这个架构就实现了无阻塞**（nonblocking）。

https://blog.51cto.com/u_15057841/3856320#SpineLeaf_84

##### Spine-Leaf-ToR 实践

Spine-Leaf两级 Clos 架构中，**每个低层级的交换机（leaf）都会连接到每个高层级的交换机 （spine），形成一个 full-mesh 拓扑**。leaf 层由接入交换机组成，用于连接服务器等 设备。spine 层是网络的骨干（backbone），负责将所有的 leaf 连接起来。 fabric 中的每个 leaf 都会连接到每个 spine，如果一个 spine 挂了，数据中心的吞吐性 能只会有轻微的下降（slightly degrade）。

如果某个链路被打满了，扩容过程也很直接：添加一个 spine 交换机就可以扩展每个 leaf 的上行链路，增大了 leaf 和 spine 之间的带宽，缓解了链路被打爆的问题。如果接入层 的端口数量成为了瓶颈，那就直接添加一个新的 leaf，然后将其连接到每个 spine 并做相 应的配置即可。这种易于扩展（ease of expansion）的特性优化了 IT 部门扩展网络的过 程。**leaf 层的接入端口和上行链路都没有瓶颈时，这个架构就实现了无阻塞**（nonblocking）。

**在 Spine-and-Leaf 架构中，任意一个服务器到另一个服务器的连接，都会经过相同数量 的设备（除非这两个服务器在同一 leaf 下面），这保证了延迟是可预测的**，因为一个包 只需要经过一个 spine 和另一个 leaf 就可以到达目的端。

叶脊架构最早 Facebook 提出，由于其更加适应全球超大型数据中心发展趋势，现在广泛应用于全球各大数据中心。

叶脊架构可以划分为机柜层、Leaf 层和 Spine 层，所用 IT 设备分别为，机柜层（服务器、ToR 交换机、光模块）；Leaf 层（Leaf 交换机、光模块）；Spine 层：（Spine交换机、光模块）。

叶脊架构主要以 Server 到 Server 之间互联流量为主，为了实现内部互联，如机柜间互联以及 Leaf-Spine 互联的短距高速高模块需求大幅增加。而 Server 到 Spine 交换机流量压力得到很大缓解，数据中心设计往往向上也呈现一定的收敛比。与此同时，交换机也伴随升级：端口数量越来越多，芯片转发速率越来越高。

机柜层：服务器、ToR 交换机、光模块；

Leaf 层：Leaf 交换机、光模块；

Spine 层：Spine 交换机、光模块；

**机柜层: **机柜层是整个网络拓扑架构中最底层的结构，用于放置服务器，通常来说单机柜可以放置 20-24 台标准服务器。机柜内部的服务器互联通过机柜顶层的交换机，该交换机也被称为 ToR（Top of Rack）交换机。实际上 TOR 交换机既可以部署在机柜顶部，也可以部署在机柜的中部 （Middle of Rack）或底部 （Bottom of Rack）。通常而言，将交换机部署在机柜顶部是最有利于走线的，因此这种架构应用最多。

**Leaf层:** Leaf 层是网络架构中承上启下的一层，主要由叶交换机组成。叶交换机向下与 ToR交换机相连，向上与脊交换机相连。每台叶交换机则向下连接 48 台 ToR 交换机，这样通过叶交换机便实现了机柜之间的互联。48 台 ToR 交换机与 4 台叶交换机对应划分出的结构也被称为 Server Pods。数据中心在部署时根据需求，划分为 N 个 Server Pods， N 的数量可以从几十到几百。边缘的 Pod 又被称为 Edge 平面，负责出口流量，实现数据中心之间的互联。

**Spine层: **Spine 层是整个数据中心拓扑网络的顶层。为了实现全网的连通性，Facebook 设计了四个独立的骨干交换机平面(Spine plane)，每个平面可以根据需求扩展脊交换机，而每个 Pod 的每一台叶交换机都会与所在平面的每一台脊交换机互联。

##### 实践部署

据数据中心内部服务器、交换机、光模块等几类 IT基础产品的用量进行详细拆解。核心假设：

\1. 高功耗单机柜满载 24 台服务器； 

\2. ToR 交换机上下行端口比例为 1:6； 

\3. Leaf 交换机上下行端口比例为 1:6；

\4. Spine 交换机不考虑上行，不考虑数据中心之间互联的 Edge 交换机

\5. 目前海外主流光模块方案，服务器互联采用 25G 光模块，ToR 与 Leaf 交换机、Leaf 与 Spine交换机采用 100G 光模块。



**机柜层：**服务器和交换机用量较为清晰，单机柜设计的服务器数量为 24 台，机柜顶放置 1 台 ToR 交换机。交换机分为下行接口和上行接口。下行即连接下层的服务器，上行即连接上层的叶交换机，中间通过两端带有光模块的 MTP/MPO 光纤连接器进行连接。光模块用量按照如下测算：

下行的测算：24（24 台服务器，一台服务器连一个光模块）+ 24（ToR 交换机 24 个下行接口）= 48 个，一般采用为 25G （25G/100G带宽）连接。上行的测算：一台 ToR 交换机一般 4 个上行接口（连接 4 个叶交换机），上行接口速率为100G。

**小结: **按照单机柜折算，即需要 24 台服务器，1 台 ToR 交换机，48 个 25G 光模块，4 个 100G高速光模块。



**Leaf层: **不含服务器，只计算交换机与光模块用量。交换机用量较为清晰，一个 Pod（48 个机柜）对应 4 台叶交换机(leaf switch)。光模块用量按照如下测算：下行的测算：

对应于 ToR 交换机的上行，一台叶交换机有 48 个下行速率为 100G 的端口，即需要 48 个 100G 的光模块。上行的测算：考虑到在 Leaf 层南北向流量较少，可以按照 1：6 的收敛比进行设计，即 8 个上行接口，Spine 平面设计 8 台脊交换机。因此单台叶交换机最多对应 6 个 100G 光模块。

**小结: **按照单机柜折算，即需要 4/48=0.08 台叶交换机，（48+8）*（4/48）=4.7 个 100G 高速光模块。



**Spine层: **按照 4 个 Spine 平面，每个平面扩展 8 个脊交换机，脊交换机总用量为 32 台；按照 64 个 Pod 平面估算，每个 Pod 平面对应 0.5 台脊交换机。光模块用量的估算，每台脊交换机下行与每个 Pod 所在平面的叶交换机相连，则要求脊交换机要有 64 个下行端口，每台脊交换机对应高速光模块用量 64 个。



**小结: **按照单机柜折算，即需要 0.5/48=0.01 台脊交换机，64*（0.5/48）=0.67 个 100G 高速光模块。



将以上汇总后，合计单机柜对应服务器 24 台、1 台 ToR 交换机，1/12 台叶交换机，1/96 台脊交换机、48 个 25G，28/3 个 100G 更高速光模块，具体用量如下表所示：

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/spine-leaf-tor.jpg)



### About the Leaf-Spine Fabric： 主流

Aruba Fabric Composer now supports two types of fabrics;

- Management Fabrics
- Data Fabrics (previously called as Leaf-Spine Fabrics)

Management Fabrics provide L2 Spine-Leaf workflows with the Aruba 6300 switch family.

The Data Fabrics consists of the following:

- **Leaf switches**: Each rack contains two leaf switches. These switches contain access ports that connect to servers, firewalls, load balancers, and edge routers within the rack. Each leaf switch has high-speed connections to the spine switches for the fabric. Pairs of leaf-switches are interconnected together using Aruba VSX technology with active LACP pairing to deliver superior server connection redundancy.
- **Spine switches**: Each rack can connect to all spine switches and each fabric supports two-to-eight spine switches. The spine switches can be located inside or outside the racks. The spine switches connect the leaf and border leaf switches, forming the leaf-spine fabric. The spine layer is made up of switches that serve as the backbone of the network. The spine switches do not connect to each other: however each spine switch does connect to all leaf and border leaf switches within the fabric.
- **Border leaf switches**: Border leaf switches provide Layer 2 or Layer 3 external connectivity to networks outside the fabric and outside the rack. Additionally, border leaf switches can connect to servers within the rack. Border leaf switches support routing protocols that exchange routes with external routers. These switches apply and enforce policies for traffic between internal and external endpoints.

The leaf-spine fabric uses dynamic Layer 3 routing protocols to determine the best path. This type of network supports data center architectures with a focus on East-West network traffic where data travels inside a data center.



### 数据中心流量： 写的真棒

https://xueyp.github.io/%E5%A4%A7%E6%95%B0%E6%8D%AE/2018/09/12/%E5%A4%A7%E6%95%B0%E6%8D%AE-%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B.html

数据中心的流量总的来说可以分为以下几种：

- 南北向流量：数据中心之外的客户端到数据中心服务器之间的流量，或者数据中心服务器访问互联网的流量。
- 东西向流量：数据中心内的服务器之间的流量。
- 跨数据中心流量：跨数据中心的流量，例如数据中心之间的灾备，私有云和公有云之间的通讯。

在这些需求里面，**更高的东西向流量支持尤为重要**。

但是，传统三层网络架构的诞生是在.com时代，主要是也为了南北向流量设计的。并非不支持东西向流量，主要有以下几种形式：

东西向的L2流量，如果源和目的主机都在同一个接入层交换机下，那么可以达到全速，因为接入交换机就能完成转发。

如果需要跨机架，但仍然是在一个汇聚层POD内，则需要通过汇聚层交换机进行转发，带宽取决于汇聚层交换机的转发速率，端口带宽和同时有多少个接入层交换机共享汇聚层交换机。前面说过汇聚层和接入层之间一般使用STP，这使得一个汇聚层POD只能有一个汇聚层交换机在工作。为了满足跨机架的L2转发，汇聚层交换机的性能，例如带宽，转发速率必然要大于接入层交换机。

如果L2流量需要跨汇聚层POD（大二层架构），那必须经过核心交换机。同样的问题仍然存在，对核心交换机的要求会更高。

东西向的L3流量，不论是不是在一个接入层交换机下，都需要走到具有L3功能的核心交换机才能完成转发。

总的来说，为了保证任意的东西向流量带宽，势必 **需要更高性能的汇聚层交换机和核心交换机**。另一方面，也可以小心的进行设计，尽量将有东西向流量的服务器置于同一个接入交换机下。不管怎么样，这都 **增加了成本，降低了可用性**。

传统的三层网络架构必然不会在短期内消失，但是由于技术和市场的发展，其短板也越来越明显。基于现有网络架构的改进显得非常有必要，新的网络架构最好是： **由相对较小规模的交换机构成，可以方便的水平扩展，较好的支持HA（active-active模式），支持全速的东西向流量，不采购高性能的核心交换机也能去除超占比，支持SDN等等**。

#### 华为数据中心网络设计

https://support.huawei.com/enterprise/zh/doc/EDOC1100023543?section=j00p

### 机架式服务器布线

https://cn.fs.com/blog/23449.html

#### EOR： End of Row

以EOR架构安装的交换机称为EOR交换机，被集中放置在网络机柜中。EOR架构是基于每排而不是每个机柜进行管理，它对EOR交换机提出了更高的要求，一但EOR交换机出现故障，影响的将是一排服务器，一般来说，框式交换机比盒式交换机更适合用在EOR结构，框式交换机在以下方面具有明显的优势：

- 更大的灵活性，框式交换机可配置不同速率和不同数量的接口板，为不同数据中心服务器的接入提供了灵活的选择；

- 更高的可靠性，框式交换机具有多电源模块、多风扇模块、多交换网板等特点，面对整排机柜的接入量具有更高的可靠性；

- 更强的扩展性，框式交换机可更换速率接口，当数据中心接入速率升级时，只用更换速率板，降低设备更迭成本

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/EOR.png)



#### MOR: Middle of Row

MOR布线是对EOR布线方式的改进，主要区别是在摆放列头机柜的位置，MOR是将其放在每一列机柜的中间 。MOR方式的网络机柜部署在POD的两排机柜的中部，由此可以减少从服务器机柜到网络机柜的线缆距离，简化线缆管理维护工作。MOR的设置方式可以使得线缆从中间位置的列柜向两端布放，降低线缆在布线通道出入口的拥堵现象，并减少线缆的平均长度，也适合实施定制长度的预连接系统，而且对布线机柜内配线设备的交叉连接和管理较EOR 要方便 。

#### TOR： Top of Rack

TOR交换机仍然可充当脊叶结构中的叶交换机，连接至骨干交换机，实现网络结构从三层到二层的完美过渡。

TOR架构是对EOR架构的扩展，它在每个服务器机柜上部署1-2台接入式交换机，服务器通过线缆接入到机柜内的交换机，交换机的上行端口通过线缆接入到网络机柜中的汇聚交换机。

TOR交换机为安装在机柜顶部的交换机，但实际上TOR交换机既可以安装在机柜顶部，也可以安装在中部或者底部，因为安装在顶部最利于走线，所以安装在顶部比较常见而已。由于安装在服务器机柜内，体积越小越好，因此TOR交换机一般采用1U-2U的盒式交换机。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/TOR.png)



##### 对比

- EoR的缺点：EoR架构将接入交换机集中放置在1～2个机柜中，方便了管理和维护，但同时也增加了服务器机柜到网络机柜之间的连线。服务器机柜离网络机柜越远，在机房中的布线距离越长，由此导致线缆维护工作量大，灵活性差。
- ToR的缺点：每个服务器机柜受电源输出功率限制，可部署的服务器数量有限，由此导致机柜内交换机的接入端口利用率不足。在几个服务器机柜间共用1-2台接入交换机，可解决交换机端口利用率不足的问题，但这种方式增加了线缆管理工作量。
- 从网络设计考虑，ToR布线方式的每台接入交换机上的VLAN量不会很多，在网络规划的时候也要尽量避免使一个VLAN通过汇聚交换机跨多台接入交换机，因此采用ToR布线方式的网络拓扑中，每个VLAN的范围不会太大，包含的端口数量不会太多。但对于EoR布线方式来说，接入交换机的端口密度高，在网路最初设计时，就可能存在包含较多端口数的VLAN。
- ToR方式的接入交换机数量多，EoR方式的接入交换机数量少，所以ToR方式的网络设备管理维护工作量大。

### 带外 and 带内网络

从技术的角度，网络管理可分为带外管理（out-of-band）和带内管理（in-band）两种管理模式。所谓带内管理，是指网络的管理控制信息与用户网络的承载业务信息通过同一个逻辑信道传送；而在带外管理模式中，网络的管理控制信息与用户网络的承载业务信息在不同的逻辑信道传送。

占用生产网络带宽、接口的管理方式称之为带内管理，与此相反，不占生产网络带宽、接口的管理方式称之为带外管理。

简单的来说,象 Openview、CiscoWorks、Tivoli 这类的网管软件系统都是带内网管，网管系统必须通过网络来管理设备。如果无法通过网络访问被管理对象，带内网管系统就失效了，这时候带外网管系统就排上用场了。就象你家里的固定电话坏了，只能通过手机来报修一样，带外网管是网管系统中的紧急通道。



带外网管是指通过专门的网管通道实现对网络的管理，将网管数据与业务数据分开，为网管数据建立独立通道。在这个通道中，只传输管理数据、统计信息、计费信息等，网管数据与业务数据分离，可以提高网管的效率与可靠性，也有利于提高网管数据的安全性。

带内管理使得网络中的网管数据和业务数据在相同的链路中传输，当管理数据（包括SNMP，Netflow，Radius，计费等）较多时，将会影响到整个网络的性能；管理数据的流量较少，对整个网络的性能影响不明显，可采用带内管理。



### 存储前后端网络

其实这就是陆哥设计的三网分离：

Ceph集群三张网卡：

* 业务流量（南北）：对外提供存储服务，即前端网络，万兆网卡
* 集群流量（东西）：ceph集群内部数据同步，即后端网络，万兆网卡
* 管理流量：administrator连接维护ceph集群的网卡，可以和后端网络复用。

### 虚拟化 and 网络

假设，网络已经高可用了，那么接入多个网络的服务器也可以实现高可用了。对服务器来说，网络高可用的代价是至少消耗两个网卡即实现bond_mode_1(active-backup)。

如下，服务网卡可以做bond，接两个交换机的网线，实现服务网络高可用。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/DRNI-1.png)

如果，没有虚拟化技术，那么每个物理设备都需要做这个的高可用，才能实现虚拟主机的网络高可用，配置和成本都很高。

在虚拟化技术的加持下，只需要在一个物理服务器上配置网络高可用，该host上所有的虚拟机都可以复用host上的网卡。即，可以实现所以vhost的网络高可用。



### 引用

1. https://www.zhihu.com/question/48343492
1. https://www.cnblogs.com/bakari/archive/2012/08/05/2623780.html