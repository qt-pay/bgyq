## 双活负载均衡架构

### Why HA？

高可用性则是为了保证应用的尽快恢复。 

### 从主备到双活的设计

双活方案是灾备要求中级别要求最高的方案，所以双活方案的部署对应用、网络、存储、虚拟化等是有一些基本的要求。

 

如果业务应用只能实现主备且同城两个数据中心以主备模式部署，那就没必要跨数据中心存储双活，还是以主备为主，这样能将生产中心的影响降到最小化，没必要为了双活而双活，一切的技术都是为了支撑上层应用和业务发展。

应用如果做了主备设计（平时使用业务生产，灾备有计算资源可供切换），无需双活存储，存储复制灾备即可、

AP 双活通过将业务分类，部分业务以数据中心 A 为主，数据中心 B 为热备，而部分业务则以数据中心 B 为主，数据中心 A 为热备，以达到近似双活的效果。

双活的实现，需要从存储、应用和网络三个方面考虑。

#### 波分设备

双活方案对距离要求，由于双活数据采用双写机制保障数据强一致性，所以一般应用可接受的距离是同城100-300KM之间，虽然应用对IO延时和超时可以设置，但是我们还得考虑用户体验。数据同步链路一般采用FC交换机级联，当两数据中心直线距离大于30KM以后，需要DWDM波分等设备来对光信号进行中继，色散补偿。一般DWDM波分设备最大支持3000KM距离。

#### AP and AA

真双活（ActiveAtive）和伪双活（ActivePassive），很多厂商讲的真双活就是两个数据中心存储上的一对镜像LUN设备，可以同时在两个数据中心接收来自一个集群应用的读写IO，数据的一致性需要存储双活集群和应用集群来保证。实际上这跟存储和应用类型关系很大，要求存储双活和应用双活都支持真双活才有意义。如果存储支持真双活，而应用是VMware（并非Active Active集群），那整个方案也只能是Active Passive模式。



#### 存储双活

若未来1-2年要改造成双活中心，可以先做存储双活。存储双活也不是一句两句就说的完的，你的裸纤，波分到位了嘛，第三站点仲裁机房到位了嘛。

建议选择同步复制（数据复制是在向主机返回写请求确认信号之前实时进行的。）的方式，要用到这份数据的时候，对卷做一次快照，挂载给计算资源使用即可。

存储双活是一种扩展HA技术，并不是完全实现容灾，当软件集群bug、裸纤故障或者存储坏块也就麻烦了，因为你的数据从逻辑层面讲是一份；还得有备份与cdp。

存储双活架构，为两个数据中心存储同时提供读写服务，且整个存储系统架构全冗余，任意数据中心故障时，另外一个数据中心有一份存储设备和相同数据可用，最大化提高了业务连续性。

#### 应用/接入层双活

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/应用层双活.webp)

其他解释： 

**1、在两个数据中心边界部署GSLB，在单数据中心全部中断服务情况下，秒级切换。**GSLB代替用户原来的Local DNS，将用户所有域名迁移到GSLB设备，由GSLB设备完成普通或者智能DNS解析。当生产中心GSLB不响应DNS时，会自动递归查询至灾备中心的GSLB，从而由灾备中心GSLB应答DNS请求，整体切换时间可达秒级。要求应用基于DNS。

 **2、在单数据中心内部署两台SLB，当单SLB中断或某单服务器中断时，仍能正常工作**。由于SLB双机部署，备机实时备份会话，当SLB-1机时，流量瞬间切换SLB-2接管，业务无影响，切换时间为秒级。

 **3、在单数据中心服务器全部中断时，通过GSLB、数据中心间二层波层链路，仍能正常工作**。

​        新用户：GSLB实时模拟用户对SLB VIP做健康监测，当获知服务器全部宕机的情况时，会立即更新DNS响应策略；此时，新用户发起的DNS请求都会得到灾备中心的VIP-B，从而实现双中心秒级业务切换。

​        老用户：由于用户访问的流程是先向GSLB请求VIP，当获取一个VIP后，就会直接通过VIP访问，在DNS失效前不再请求新的DNS。由此，在生产中心服务器宕机的短时间内，会存在一个情况，即：GSLB此时通过健康检测，能够立即获知生产中心服务器集群不可用，且会立即更新DNS相应策略（相应灾备中心VIP-B）；但仍然有用户暂未更新VIP，此时仍然会访问生产中心VIP-A；解决方法是：配置VIP-B作为主中心VIP-A下挂服务器组的备份组，当所有服务器集群不可用时，前来访问的用户流量会被生产中心SLB-1引流至灾备中心SLB-1，以此来保证业务流量不中断，整体切换时间可达秒级。

#### 网络双活

跨中心的大二层网络，是网络双活的点。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/跨中心的二层网络.webp)

#### 虚拟机双活

虚拟机HA框架的示意图,实现该框架需满足以下要求：

（1） 需要两台物理服务器。

（2） 每台物理服务器都安装好KVM虚拟化系统。

（3） 每台物理服务器需要两块ISCSI HBA网卡,一块用作应用服务网桥接口,另一块将两台服务器相连用作虚拟机的心跳网桥接口。

（4） 物理服务器必须连接到同一个共享存储设备（LUN）。

​            每个物理服务器必须连接两个以上共享LUN，避免一个LUN挂掉后，导致集群故障。

（5） 选择用作高可用集群的两台虚拟机（VM）必须位于不同的物理服务器（PM）。



#### 云平台双活

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/云结构下双活.webp)



### VIP 32 bits mask and lo interface

当前主机的路由表...

```bash
Destination 	Gateway	       Genmask	          Use Iface
192.168.247.0	0.0.0.0	       255.255.255.0	   lo
192.168.247.0	0.0.0.0	       255.255.255.0	   eth0
0.0.0.0	       192.168.247.2	0.0.0.0	           eth0
```

（Gateway是0.0.0.0或者*表示目标是本主机所属的网络，不需要路由）
注意：上面的lo是我假设的，为了方便下面的说明，实际在Linux中`route -n`是看不到lo接口的设备的，因为它不对外。

假设当前目标IP 192.168.247.1，**VIP**为192.168.247.100

lo网卡 192.168.247.100 & 255.255.255.0 = 192.168.247.0
eth0网卡 192.168.247.12 & 255.255.255.0 = 192.168.247.0
目标IP地址 192.168.247.1 & 255.255.255.0=192.168.247.0

然后我们需要明白一个事，叫**IP最长匹配原则**。因此不会走最后的默认网关192.168.247.2。而且由于&运算发现目标IP和主机在同一网段内，因此会走上面两条，而且上面两条其实都会匹配成功，但是，**由于是环回网卡一种特殊的网络接口，不与任何实际设备连接，而是完全由软件实现。而且lo环回网卡离内核近，所以在路由表中为第一条，所以数据包会走lo环回接口。**

> 路由最长匹配原则，“能够完全匹配的最长”的，在能够匹配的情况下，越长越精确。

而环回接口有一个特点，就是接收到的数据包又会发回给本机，也就是说回环网卡是自己和自己玩，因此如果走的是环回接口发送数据包，永远也发不出去，因此我们不能让数据包走环回接口，所以需要将掩码设置成255.255.255.255，这样&运算192.168.247.1  ！=192.168.247.100

```bash
Destination   	Gateway	   Genmask	          Use Iface
192.168.247.0	0.0.0.0	   255.255.255.255	   lo
192.168.247.0	0.0.0.0	   255.255.255.0	   eth0
0.0.0.0	     192.168.247.2	0.0.0.0	           eth0
```

因此就不会走我们的环回接口发送数据包，而是走eth0发送数据包。然后通过eth0向交换机广播，获取192.168.247.1地址的MAC地址，将数据包发送过去就完成了相应的过程。

### GSLB：基于DNS

GSLB, Global Server Load Balance, 即全局负载均衡。

由于现实中存在各种不稳定因素，比如某个服务器集群所在的数据中心断电，洪水或者地震造成数据中心瘫痪等等。在一个数据中心内，无论采用怎样的技术，总可能存在一些不可抗因素，导致其瘫痪。所以通常会把服务器分散部署到多个数据中心，以最大程度减小灾害对服务质量产生影响的概率和程度。

比如，CDN系统总是希望用距离用户最近的设备为其提供服务，这也需要在不同地域部署多个节点。

GSLB系统就是针对这个问题的。它负责多个CDN节点之间相互协作，将各节点和设备的负载保持在一个有利于提供优质服务的水平。GSLB的负载均衡结果可能直接将用户分配到RS，也可能将用户交付到下一层次的负载均衡系统。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/gslb-dns-实现.jpg)

#### NS记录

NS(Name Server）记录是域名服务器记录，用来指定该域名由哪个DNS服务器来进行解析。

如果需要把子域名交给其他DNS服务商解析，就需要添加NS记录（Name Server）。NS记录是域名服务器记录，用来指定该域名由哪个DNS服务器来进行解析。NS记录中的IP即为该DNS服务器的IP地址。大多数域名注册商默认用自己的NS服务器来解析用户的DNS记录。DNS服务器NS记录地址一般以以下的形式出现：ns1.domain.com、ns2.domain.com等。

#### EIP and SLB



DNS 解析到两个不同权重的业务EIP地址，从而实现两个中心高可用。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/gslb-实现.png)

DNS记录作为入口，解析到EIP，EIP绑定在SLB上，SLB要实现跨AZ的实例解析。因为一个应用为了高可用会跨AZ部署。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/glb-slb-跨az.png)



#### 基于DNS是GSLB实现

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/gslb-dns.jpg)

中心控制节点配置一台GSLB Controller及数台指定域名（abc.com）的授权DNS服务器，GSLB Controller除了进行GSLB控制外还可以对DNS服务器及其他应用服务器进行负载均衡。设置2个站点（以中国电信和中国网通为例）提供应用服务。其工作流程如下：

**1)**    用户发起请求访问[http://www.abc.com](http://www.abc.com/)，关于[www.abc.com](http://www.abc.com/) 的DNS请求被送往 Local DNS服务器；

**2)**    Local DNS通过根DNS服务器查询到abc.com 的授权DNS服务器，Local DNS向授权DNS服务器发DNS请求。

**3)**    GSLB Controller 截获DNS服务器返回的应答，并基于一组策略选择最佳的站点VIP 地址，返回给Local DNS服务器。

GSLB Controller也可以根据事先定义的策略返回CNAME记录，在大规模的多级GSLB设计中会用到这种方式。Local DNS会递归发送DNS请求到负责指定CNAME域的下一级GSLB Controller。

**4)**    Local DNS服务器返回该DNS应答到用户。

**5)**    用户根据解析到的IP地址建立连接进行正常访问。

 

从GSLB处理流程可以看出，其核心在GSLB策略。接下来简单介绍一下常用的一些GSLB策略。

\1)    各内容站点的“健康状况”

\2)    地理区域或用户自定义区域

一个区域为若干条IP地址前缀。根据用户本地DNS的IP地址，将特定IP范围的用户优先分配到某个通过健康检查的站点。

\3)    IP地址权重

可以为DNS应答中的每个IP地址分配权重，权重决定与其他候选IP相比分配到该IP的流量比例。

\4)    站点（Site）权重

可以为每个Site分配权重，权重决定与其他候选Site相比分配到该Site的流量比例。

\5)    会话能力阈值

通过厂商自由的GSLB协议，GSLB Controller可以获得每个站点负载均衡设备当前可用会话数和会话表大小的最大值，当前会话数/最大会话数比值超过定义的阈值时，该站点不再被选择。

\6)    活动服务器

指一个GSLB节点绑定到一个VIP上的活动真实服务器数量。可以配置策略优先选择活动服务器最多的IP地址。

\7)    往返时间(RTT)

RTT策略是基于区域之外最常用的策略。有两种模式的RTT测量：Active RTT测量与Passive RTT测量。在实际部署中，由于网络限制和性能原因，Active RTT往往无法使用，Passive RTT更实用一些。

a)     Active RTT 测量

b) Passive RTT测量

\8)    当前可用会话数

\9)    站点管理优先级（Admin Preference）

为每个站点预设优先级，选择优先级较高的站点。

\10)      最少选择

选择从前被选择的次数最少的节点。

\11)   轮询（Round Robin）

采用轮询方式选择站点。


### 基于DNS轮训

配置两套主备模式的负载均衡器，分别配置VIP A和VIP B。在DNS server测针对同一域名，同时注册VIP A和VIP B。然后在请求域名的时候，dns通过轮训返回VIP A或者 VIP B。在服务端，需要通过memcached/redis集群来共享不同节点的内存数据。从而保证session的持久性

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/Round-robinDNS.png)



### 基于ECMP

此方案中，不存在主备模式的负载均衡器，所有负载均衡器均为主模式，配置同一VIP。在router测，利用quagga模拟ospf，可以看到到达VIP的链路有两条，当请求到达router的时候，会将流量分配到两台主负载均衡器上。

> quagga是一个实现ospf的路由软件，用于模拟ospf协议。

![](https://image-1300760561.cos.ap-beijing.myqcloud.com/bgyq-blog/ecmp-routing.png)

由于接收数据包的目的IP是VIP，所以需要将vip绑定到lo网卡，因为lo:vip不是用来通信的，所以这里的一定设置/32位子网掩码。

lo网卡是loop，不会将数据转发出去，所以需要将掩码设置成32位。当数据包进来时，会使用掩码进行判断是否属于同一子网，进而选择网关进行转发，当lo interface设置成32 bits时，就不会有数据包进入lo 接口。

### 引用

1. https://blog.csdn.net/SmallCatBaby/article/details/89876508
2. https://lk668.github.io/2020/12/13/2020-12-13-ECMP/
3. https://cloud.tencent.com/developer/article/1512154